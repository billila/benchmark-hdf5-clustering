---
title: mbkmeans manuscript figures
author: Ruoxi Liu, Stephanie Hicks, Davide Risso, Elizabeth Purdom
output: 
    html_document:
        theme: cosmo 
        toc: true
        toc_float: true
        highlight: tango
        number_sections: false
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE)
```

```{r, message=FALSE}
library(here)
library(tidyr)
library(dplyr)
library(ggplot2)
library(cowplot)
library(ggpubr)
library(gridExtra)
library(grid)
library(here)
theme_set(theme_cowplot())
```

# Load data

## Memory

### Subsampling TENxBrainData 

```{r}
files_sub <- list.files(path= here("main", "case_studies", "output"), 
                        pattern="*.csv", full.names=TRUE, recursive=FALSE)

files_sub <- files_sub[grep("TENxBrainData", files_sub)]
files_sub <- files_sub[grep("k_davide_mac", files_sub)]
files_sub_mem <- files_sub[grep("memory", files_sub)]

sub_mem_table <- NULL

for (i in (1:length(files_sub_mem))){
  temp_table <- read.csv(file = files_sub_mem[i], header=FALSE, sep=",")
  sub_mem_table <- rbind(sub_mem_table, temp_table)
}

colnames(sub_mem_table) <- c("Dataset", "machine", "ncells", "ngenes", "step",
                             "method", "batch_size", "B", "memory")
sub_mem_table <- sub_mem_table %>% 
  mutate(method = factor(method, levels = c("kmeans", "mbkmeans", "hdf5")),
         batch = round(batch_size * ncells),
         memory = memory/1000) %>% 
  dplyr::rename(Algorithm = method)

levels(sub_mem_table$Algorithm) <- c("k-means", "mbkmeans", "mbkmeans (HDF5)")
```

## Time 

### Subsampling TENxBrainData 

```{r}
files_sub_time <- files_sub[grep("time", files_sub)]

sub_time_table <- NULL

for (i in (1:length(files_sub_time))){
  temp_table <- read.csv(file = files_sub_time[i], header=FALSE, sep=",")
  sub_time_table <- rbind(sub_time_table, temp_table)
}

colnames(sub_time_table) <- c("Dataset", "machine", "ncells", "ngenes", "step",
                             "method", "batch_size", "B", "user", "system",
                             "elapsed")
sub_time_table <- sub_time_table %>% 
  mutate(method = factor(method, levels = c("kmeans", "mbkmeans", "hdf5")),
         batch = round(batch_size * ncells)) %>% 
  dplyr::rename(Algorithm = method)

levels(sub_time_table$Algorithm) <- c("k-means", "mbkmeans", "mbkmeans (HDF5)")

```


## Accuracy

Here we are using the performance metrics adjusted Rand index (ARI) and 
within-clusters sum of squares (WCSS). 
All assessments in this section use absolute batch sizes.

### Simulated data

#### Fixed $k$

```{r}
files_acc <- list.files(path = here("output_tables", "abs_batch", "acc"), 
                        pattern="*.csv", full.names=TRUE, recursive=FALSE)
acc_table_abs <- NULL
for (i in (1:length(files_acc))){
  temp_table <- read.csv(file = files_acc[i], header=TRUE, sep=",")
  acc_table_abs <- rbind(acc_table_abs, temp_table)
}
acc_table_abs <- acc_table_abs %>% 
  transform(normal_WCSS = WCSS/observations) %>% 
  dplyr::rename(Algorithm = method)
acc_table_abs$Algorithm <- factor(acc_table_abs$Algorithm, 
                               levels = c("kmeans", "mbkmeans", "hdf5"))
levels(acc_table_abs$Algorithm) <- c("k-means", "mbkmeans", "mbkmeans (HDF5)")
```

#### Varying $k$

```{r}
files_vark <- list.files(path = here("output_tables", "Varying_k", "acc"), 
                        pattern="*.csv", full.names=TRUE, recursive=FALSE)
vark_table_abs <- NULL
for (i in (1:length(files_vark))){
  temp_table <- read.csv(file = files_vark[i], header=TRUE, sep=",")
  vark_table_abs <- rbind(vark_table_abs, temp_table)
}

vark_table_abs <- vark_table_abs %>% 
  transform(normal_WCSS = WCSS/observations) %>% 
  dplyr::rename(Algorithm = method)
vark_table_abs$Algorithm <- factor(vark_table_abs$Algorithm, 
                               levels = c("kmeans", "mbkmeans", "hdf5"))
levels(vark_table_abs$Algorithm) <- c("k-means", "mbkmeans", "mbkmeans (HDF5)")
```

### Subsampling TENxBrainData 

#### Fixed $k$

```{r}
files_acc <- list.files(path = here("output_tables", "abs_batch", 
                                    "acc", "real_data"), 
                        pattern="*.csv", full.names=TRUE, recursive=FALSE)
acc_real_table_abs <- NULL
for (i in (1:length(files_acc))){
  temp_table <- read.csv(file = files_acc[i], header=TRUE, sep=",")
  acc_real_table_abs <- rbind(acc_real_table_abs, temp_table)
}
acc_real_table_abs <- acc_real_table_abs %>% 
  transform(normal_WCSS = WCSS/observations) %>% 
  dplyr::rename(Algorithm = method)
acc_real_table_abs$Algorithm <- factor(acc_real_table_abs$Algorithm, 
                               levels = c("kmeans", "mbkmeans", "hdf5"))
levels(acc_real_table_abs$Algorithm) <- c("k-means", "mbkmeans", "mbkmeans (HDF5)")
```


# Figures 

## Figure 1

Here we select one dataset size ($N$ = XXX) by downsampling from the 
`TENxBrainData` and show the impact of memory and time with mbkmeans and 
kmeans using a specific $k=15$ and batch size (`batch` = 500). 

### Time

```{r}
sub_time_plot <- sub_time_table %>% 
  dplyr::filter(Algorithm == "k-means" | batch == 500) %>%
  dplyr::group_by(Algorithm) %>% 
  ggplot(aes(x = ncells, y = elapsed / 60, color = Algorithm)) +
          geom_line() +
          geom_point(size = 3) +
      labs(title = "Elapsed Time for increasing number of cells") + 
        xlab("Number of cells") + 
        ylab("Elapsed time (mins)") + 
  theme(legend.position = "top", legend.justification= "center")
sub_time_plot
```

### Memory

```{r}
sub_mem_plot <- sub_mem_table %>% 
  dplyr::filter(Algorithm == "k-means" | batch == 500) %>%
  dplyr::group_by(Algorithm) %>% 
  ggplot(aes(x = ncells, y = memory, color = Algorithm)) +
          geom_line()+
          geom_point(size = 3)+
      labs(title = "Memory Usage (RAM) for increasing number of cells") + 
        xlab("Number of cells") + 
        ylab("Memory Usage (Gb)") + 
  theme(legend.position = "top", legend.justification= "center")
sub_mem_plot
```

```{r}
p1 <- ggdraw() +
    draw_plot(sub_time_plot, x = 0, y = 0.51, width = 1, height = .45) +
    draw_plot(sub_mem_plot, x = 0, y = 0, width = 1, height = .45) +
    draw_plot_label(label = LETTERS[1:2], size = 25,
                    x = c(0, 0), y = c(1, 0.53))
    
p1

pdf(here::here("main", "figs", "fig1.pdf"), width = 10, height =10)
print(p1)
dev.off()
```

## Figure 2

Here we select three sizes of datasets ($N$) and show the accuracy 
as a function of absolute batch size using both simulated and 
real scRNA-seq data. 

### ARI 
```{r}
p_ari_obs_line_abs <- acc_table_abs %>% 
  dplyr::filter((observations %in% c(5000,10000,25000)))%>%
  dplyr::group_by(Algorithm, observations, abs_batch) %>% 
  dplyr::summarize(mean = mean(ARI), 
                   sd = sd(ARI)) %>% 
  ggplot(aes(x = abs_batch, y = mean, color = Algorithm)) +
          geom_line() +
          geom_point() +
          geom_errorbar(aes(ymin=mean-sd, ymax=mean+sd), width=.2,
                position=position_dodge(0.05)) +
      ylim(0.95, 1) + 
      labs(title = "Performance of accuracy with three simulated scRNA-seq datasets") + 
        xlab("Absolute batch sizes") + 
        ylab("ARI") + 
  theme(legend.position = "top", legend.justification= "center") +
  facet_wrap(~observations, scales = "free") 
p_ari_obs_line_abs
```

Save the legend on its own
```{r}
my_legend <- cowplot::get_legend(p_ari_obs_line_abs)
```

Resave figure without legend

```{r}
p_ari_obs_line_abs <- 
  p_ari_obs_line_abs + 
  theme(legend.position = "none", legend.justification= "center")
```



### WCSS
```{r}
p_wcss_obs_line_abs <- acc_table_abs %>% 
  dplyr::filter((observations %in% c(5000,10000,25000)))%>%
  dplyr::group_by(Algorithm, observations, abs_batch) %>% 
  mutate(q90 = quantile(normal_WCSS, 0.9), row = row_number()) %>% 
  dplyr::filter(normal_WCSS <= q90) %>%
  dplyr::summarize(mean = mean(normal_WCSS), 
                   sd = sd(normal_WCSS)) %>% 
  ggplot(aes(x = abs_batch, y = mean, color = Algorithm)) +
          geom_line() +
          geom_point() +
          geom_errorbar(aes(ymin=mean-sd, ymax=mean+sd), width=.2,
                position=position_dodge(0.05))+
     labs(title = "Performance of accuracy with three simulated scRNA-seq datasets") + 
        xlab("Absolute batch sizes") + 
        ylab("WCSS") + 
  theme(legend.position = "none", legend.justification= "center") +
  facet_wrap(~observations, scales = "free") 
p_wcss_obs_line_abs
```

```{r}
p_wcss_real_abs <- acc_real_table_abs %>%   
  dplyr::filter((observations %in% c(5000,10000,25000)))%>%
  dplyr::group_by(observations, Algorithm, abs_batch) %>% 
  mutate(q90 = quantile(normal_WCSS, 0.8), row = row_number()) %>% 
  dplyr::filter(normal_WCSS <= q90) %>%
  dplyr::summarize(mean = median(normal_WCSS), 
                   sd = sd(normal_WCSS)) %>% 
  ggplot(aes(x = abs_batch, y = mean, color = Algorithm)) +
          geom_line()+
          geom_point()+
          geom_errorbar(aes(ymin=mean-sd, ymax=mean+sd), width=.2,
                position=position_dodge(0.05))+
  theme(legend.position = "none", legend.justification= "center") +
  labs(title = "Performance of accuracy with three real scRNA-seq datasets") + 
        xlab("Absolute batch sizes") + 
        ylab("WCSS") +
  facet_wrap(~observations, scales = "free") 
p_wcss_real_abs
```

### save fig

```{r}
p2_abs <- ggdraw() +
    draw_plot(as_ggplot(my_legend), x = 0, y = 0.94, width = 1, height = 0.05) + 
    draw_plot(p_ari_obs_line_abs, x = 0, y = 0.62, width = 1, height = .32) +
    draw_plot(p_wcss_obs_line_abs, x = 0, y = 0.31, width = 1, height = .31) +
    draw_plot(p_wcss_real_abs, x = 0, y = 0, width = 1, height = .31) +
    draw_plot_label(label = LETTERS[1:3], size = 25,
                    x = c(0, 0, 0), y = c(1, 0.64, 0.33))
    
p2_abs
pdf(here::here("main", "figs", "fig2.pdf"), width = 10, height =10)
print(p2_abs)
dev.off()
```


## Figure 3

Here we select one size of datasets ($N$) and show role of 
batch size on time and memory using real scRNA-seq data. 
as a function of absolute batch size using both simulated and 
real scRNA-seq data. 



## Figure 4

HDF5 geometry (real data). Best, worst, default. K=15

## Figure 5

Add here. 




# Supplemental Figures

## Simulated data (all N)
All assessments in this section use absolute batch sizes.

### Accuracy (ARI)

#### Fixed $k$

```{r}
p_ari_obs_line_abs_all <- acc_table_abs %>% 
  dplyr::group_by(Algorithm, observations, abs_batch) %>% 
  dplyr::summarize(mean = mean(ARI), 
                   sd = sd(ARI)) %>% 
  ggplot(aes(x = abs_batch, y = mean, color = Algorithm)) +
          geom_line() +
          geom_point() +
          geom_errorbar(aes(ymin=mean-sd, ymax=mean+sd), width=.2,
                position=position_dodge(0.05))+
      ylim(0.95, 1) + 
      labs(title = "Performance of accuracy with increasing sizes of simulated scRNA-seq datasets") + 
        xlab("Absolute batch sizes") + 
        ylab("adjusted Rand index (ARI)") + 
  theme(legend.position = "top", legend.justification= "center") +
  facet_wrap(~observations, scales = "free") 
p_ari_obs_line_abs_all
```

```{r}
pdf(here::here("main", "figs", "fig-supp-sim-ari.pdf"), width = 10, height =10)
print(p_ari_obs_line_abs_all)
dev.off()
```

#### Varying $k$ 

```{r}
p_vark_ari <- vark_table_abs %>% 
  dplyr::group_by(Algorithm, observations, batch_size, k) %>% 
  dplyr::summarize(mean = mean(ARI), 
                   sd = sd(ARI)) %>% 
  ggplot(aes(x = k, y = mean, color = Algorithm)) +
          geom_line() +
          geom_point() +
          geom_errorbar(aes(ymin=mean-sd, ymax=mean+sd), width=.2,
                position=position_dodge(0.05)) +
     labs(title = "Performance of accuracy with two sizes of simulated scRNA-seq datasets\nand three absolute batch sizes (75, 500, 1000) \n with the true number of clusters as k = 15") + 
        xlab("Number of clusters (k) used in clustering algorithm") + 
        ylab("adjusted Rand index (ARI)") + 
  theme(legend.position = "top", legend.justification= "center") +
  facet_grid(batch_size ~ observations, scales = "free") 
p_vark_ari
```

```{r}
pdf(here::here("main", "figs", "fig-supp-sim-ari-varyingk.pdf"), width = 10, height =10)
print(p_vark_ari)
dev.off()
```


### Accuracy (WCSS)

#### Fixed $k$

```{r}
p_wcss_obs_line_abs_all <- acc_table_abs %>% 
  dplyr::group_by(Algorithm, observations, abs_batch) %>% 
  mutate(q90 = quantile(normal_WCSS, 0.9), row = row_number()) %>% 
  dplyr::filter(normal_WCSS <= q90) %>%
  dplyr::summarize(mean = mean(normal_WCSS), 
                   sd = sd(normal_WCSS)) %>% 
  ggplot(aes(x = abs_batch, y = mean, color = Algorithm)) +
          geom_line() +
          geom_point() +
          geom_errorbar(aes(ymin=mean-sd, ymax=mean+sd), width=.2,
                position=position_dodge(0.05))+
     labs(title = "Performance of accuracy with increasing sizes of simulated scRNA-seq datasets") + 
        xlab("Absolute batch sizes") + 
        ylab("WCSS") + 
  theme(legend.position = "top", legend.justification= "center") +
  facet_wrap(~observations, scales = "free") 
p_wcss_obs_line_abs_all
```

```{r}
pdf(here::here("main", "figs", "fig-supp-sim-wcss.pdf"), width = 10, height =10)
print(p_wcss_obs_line_abs_all)
dev.off()
```

#### Varying $k$ 

Using absolute batch sizes. Fixed $k$.
```{r}
p_vark_wcss <- vark_table_abs %>% 
  dplyr::group_by(Algorithm, observations, batch_size, k) %>% 
  dplyr::summarize(mean = mean(normal_WCSS), 
                   sd = sd(normal_WCSS)) %>% 
  ggplot(aes(x = k, y = mean, color = Algorithm)) +
          geom_line() +
          geom_point() +
          geom_errorbar(aes(ymin=mean-sd, ymax=mean+sd), width=.2,
                position=position_dodge(0.05))+
     labs(title = "Performance of accuracy with two sizes of simulated scRNA-seq datasets\nand three absolute batch sizes (75, 500, 1000)\n with the true number of clusters as k = 15") + 
        xlab("Number of clusters (k) used in clustering algorithm") + 
        ylab("WCSS") + 
  theme(legend.position = "top", legend.justification= "center") +
  facet_grid(batch_size ~ observations, scales = "free") 
p_vark_wcss
```

```{r}
pdf(here::here("main", "figs", "fig-supp-sim-wcss-varyingk.pdf"), width = 10, height =10)
print(p_vark_wcss)
dev.off()
```

## TENxBrain data (all N)

### Accuracy (WCSS)

Only three sizes of datasets were assessed (5000, 10000, 25000). These 
results were shown in Figure 2. 


### Memory and time 

memory and time (under 200K cells)

Performance of memory usage as a function of increasing number of 
cells and batch sizes. 

```{r}
p_mem_obs <- mem_table %>%
  dplyr::filter((batch_size %in% c(0.005, 0.05, 0.20)))%>%
  dplyr::filter((observations <= 200000)) %>%
  dplyr::group_by(method, observations, batch_size_perc) %>% 
  dplyr::summarize(mean = mean(memory/1000), 
                   sd = sd(memory/1000)) %>% 
  ggplot(aes(x = observations/1e3, y = mean, color = method)) +
          geom_line() +
          geom_point() +
          geom_errorbar(aes(ymin=mean-sd, ymax=mean+sd), width=.2,
                position=position_dodge(0.05))+
        labs(title = "Memory Usage (RAM) for increasing number of cells and batch size") + 
        xlab("Number of cells (thousands)") + 
        ylab("Memory Usage (Gb)") + 
  facet_wrap(~batch_size_perc, scales = "free") + 
  theme(legend.position = "top", legend.justification= "center") +
  scale_color_discrete(name = "Method",
                       breaks=c("kmeans-new", "mbkmeans-new", "mbkmean_hdf5-new"),
                       labels=c("k-means", "mbkmeans", "mbkmeans (HDF5)")) 
p_mem_obs
```

```{r}
p_time_obs <- time_table %>% 
  dplyr::filter((batch_size %in% c(0.005, 0.05, 0.20)))%>%
  dplyr::filter((observations <= 200000)) %>%
  gather(key = "time_type", value = "time", -c(B:method), -batch_size_perc)  %>%
  dplyr::filter(time_type == "elapsed_time") %>%
  dplyr::group_by(method, observations, batch_size_perc, time_type) %>% 
  dplyr::summarize(mean = mean(time), 
                   sd = sd(time)) %>% 
  ggplot(aes(x = observations/1e3, y = mean, color = method)) +
          geom_line()+
          geom_point()+
          geom_errorbar(aes(ymin=mean-sd, ymax=mean+sd), width=.2,
                position=position_dodge(0.05))+
       labs(title = "Elapsed Time for increasing number of cells and batch size") + 
        xlab("Number of cells (thousands)") + 
        ylab("Time (seconds)") + 
  facet_wrap(~batch_size_perc, scales = "free") + 
  theme(legend.position = "none", legend.justification= "center") +
  scale_color_discrete(name = "Method",
                       breaks=c("kmeans-new", "mbkmeans-new", "mbkmean_hdf5-new"),
                       labels=c("k-means", "mbkmeans", "mbkmeans (HDF5)")) 
p_time_obs
```


memory and time (up to a million cells)

Performance of memory usage as a function of increasing number of 
cells and batch sizes. 

```{r}
p_mem_obs_all <- mem_table %>%
  dplyr::filter((batch_size %in% c(0.005, 0.05, 0.20)))%>%
  dplyr::filter(!(observations %in% c(250e3, 300e3, 375e3, 400e3, 450e3))) %>%
  dplyr::group_by(method, observations, batch_size_perc) %>% 
  dplyr::summarize(mean = mean(memory/1000), 
                   sd = sd(memory/1000)) %>% 
  ggplot(aes(x = observations/1e3, y = mean, color = method)) +
          geom_line() +
          geom_point() +
          geom_errorbar(aes(ymin=mean-sd, ymax=mean+sd), width=.2,
                position=position_dodge(0.05))+
        labs(title = "Memory Usage (RAM) for increasing number of cells and batch size") + 
        xlab("Number of cells (thousands)") + 
        ylab("Memory Usage (Gb)") + 
  facet_wrap(~batch_size_perc, scales = "free") + 
  theme(legend.position = "top", legend.justification= "center") +
  scale_color_discrete(name = "Method",
                       breaks=c("kmeans-new", "mbkmeans-new", "mbkmean_hdf5-new"),
                       labels=c("k-means", "mbkmeans", "mbkmeans (HDF5)")) 
p_mem_obs_all
```


```{r}
p_time_obs_all <- time_table %>% 
  dplyr::filter((batch_size %in% c(0.005, 0.05, 0.20)))%>%
  # dplyr::filter((observations <= 200000)) %>%
  gather(key = "time_type", value = "time", -c(B:method), -batch_size_perc)  %>%
  dplyr::filter(time_type == "elapsed_time") %>%
  dplyr::group_by(method, observations, batch_size_perc, time_type) %>% 
  dplyr::summarize(mean = mean(time), 
                   sd = sd(time)) %>% 
  ggplot(aes(x = observations/1e3, y = mean, color = method)) +
          geom_line()+
          geom_point()+
          geom_errorbar(aes(ymin=mean-sd, ymax=mean+sd), width=.2,
                position=position_dodge(0.05))+
       labs(title = "Elapsed Time for increasing number of cells and batch size") + 
        xlab("Number of cells (thousands)") + 
        ylab("Time (seconds)") + 
  facet_wrap(~batch_size_perc, scales = "free") + 
  theme(legend.position = "none", legend.justification= "center") +
  scale_color_discrete(name = "Method",
                       breaks=c("kmeans-new", "mbkmeans-new", "mbkmean_hdf5-new"),
                       labels=c("k-means", "mbkmeans", "mbkmeans (HDF5)")) 
p_time_obs_all
```


