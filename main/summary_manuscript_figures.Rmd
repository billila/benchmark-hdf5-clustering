---
title: mbkmeans manuscript figures
author: Ruoxi Liu, Stephanie Hicks, Davide Risso, Elizabeth Purdom
output: 
    html_document:
        theme: cosmo 
        toc: true
        toc_float: true
        highlight: tango
        number_sections: false
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE)
```

```{r, message=FALSE}
library(here)
library(tidyr)
library(dplyr)
library(ggplot2)
library(cowplot)
library(ggpubr)
library(gridExtra)
library(grid)
library(here)
theme_set(theme_cowplot())

if(!file.exists(here("main", "csv-tables"))){
  dir.create(here("main", "csv-tables"))
}
```

# Load data

## Memory

Here we are profiling memory useage in R using `Rprof()` in base R. 
All assessments in this section use absolute batch sizes.

### Subsampling TENxBrainData 

This loads in results from both Davide and Ruoxi. 

#### Increasing $b$ and $N$

```{r}
files_sub <- list.files(path= here("main", "case_studies", "output"), 
                        pattern="*.csv", full.names=TRUE, recursive=FALSE)

files_sub <- files_sub[grep("TENxBrainData", files_sub)]
files_sub <- files_sub[grep("k_ruoxi_cluster|k_davide_mac", files_sub)]
files_sub_mem <- files_sub[grep("memory", files_sub)]

sub_mem_table <- NULL

for (i in (1:length(files_sub_mem))){
  temp_table <- read.csv(file = files_sub_mem[i], header=FALSE, sep=",")
  sub_mem_table <- rbind(sub_mem_table, temp_table)
}

colnames(sub_mem_table) <- c("Dataset", "machine", "ncells", "ngenes", "step",
                             "method", "batch_size", "B", "memory")
sub_mem_table <- sub_mem_table %>% 
  mutate(method = factor(method, levels = c("kmeans", "mbkmeans", "hdf5")),
         batch = round(batch_size * ncells),
         memory = memory/1000) %>% 
  dplyr::rename(Algorithm = method)

levels(sub_mem_table$machine) <- c("desktop", "HPC cluster")
levels(sub_mem_table$Algorithm) <- c("k-means", "mbkmeans", "mbkmeans (HDF5)")
```

#### HDF5 geometry

```{r}
mem_table_davide <- 
  read.csv(here("ongoing_analysis", "ChunkTest", "TENxBrainData", 
                "Output", "mem_davide_mac.csv"), header=TRUE, sep=",")
mem_table_ruoxi <- 
  read.csv(here("ongoing_analysis", "ChunkTest", "TENxBrainData", 
                "Output", "mem_ruoxi_cluster.csv"), header=TRUE, sep=",")

mem_table_hdf5_geom <- 
  cbind(rbind(mem_table_davide, mem_table_ruoxi), 
        "machine" = c(rep("desktop", nrow(mem_table_davide)),
                    rep("HPC cluster", nrow(mem_table_ruoxi))))

mem_table_hdf5_geom <- mem_table_hdf5_geom %>% 
  dplyr::rename(chunk_size = geometry)  %>% 
  mutate(chunk_size = 
           factor(chunk_size, levels = c("best", "default",
                                         "worst", "singleChunk")), 
         machine = factor(machine, levels = c("desktop", "HPC cluster")))
levels(mem_table_hdf5_geom$chunk_size) <- 
  c("per cell (best)", "default", "per gene (worst)", "single chunk")
```

### Simulated data

This currently loads in results from only Ruoxi. 
Need to add Davide's output (should be ready by 4/29). 

#### Varying $k$

```{r}
files_vark_mem <- list.files(path = here("output_tables", "Varying_k", "mem"), 
                        pattern="*.csv", full.names=TRUE, recursive=FALSE)
vark_table_mem <- NULL
for (i in (1:length(files_vark_mem))){
  temp_table <- read.csv(file = files_vark_mem[i], header=TRUE, sep=",")
  vark_table_mem <- rbind(vark_table_mem, temp_table)
}

vark_table_mem <- vark_table_mem %>% 
  dplyr::rename(Algorithm = method)

vark_table_mem$Algorithm <- factor(vark_table_mem$Algorithm, 
                               levels = c("kmeans", "mbkmeans", "hdf5"))
levels(vark_table_mem$Algorithm) <- c("k-means", "mbkmeans", "mbkmeans (HDF5)")
```

## Time 

### Subsampling TENxBrainData 

This loads in results from both Davide and Ruoxi.

#### Increasing $b$ and $N$

```{r}
files_sub_time <- files_sub[grep("time", files_sub)]

sub_time_table <- NULL

for (i in (1:length(files_sub_time))){
  temp_table <- read.csv(file = files_sub_time[i], header=FALSE, sep=",")
  sub_time_table <- rbind(sub_time_table, temp_table)
}

colnames(sub_time_table) <- c("Dataset", "machine", "ncells", "ngenes", "step",
                             "method", "batch_size", "B", "user", "system",
                             "elapsed")
sub_time_table <- sub_time_table %>% 
  mutate(method = factor(method, levels = c("kmeans", "mbkmeans", "hdf5")),
         batch = round(batch_size * ncells)) %>% 
  dplyr::rename(Algorithm = method)

levels(sub_time_table$machine) <- c("desktop", "HPC cluster")
levels(sub_time_table$Algorithm) <- c("k-means", "mbkmeans", "mbkmeans (HDF5)")
```


#### HDF5 geometry

```{r}
time_table_davide <- 
  read.csv(here("ongoing_analysis", "ChunkTest", "TENxBrainData",
                "Output", "time_davide_mac.csv"), header=TRUE, sep=",")
time_table_ruoxi <- 
  read.csv(here("ongoing_analysis", "ChunkTest", "TENxBrainData",
                "Output", "time_ruoxi_cluster.csv"), header=TRUE, sep=",")

time_table_hdf5_geom <- 
  cbind(rbind(time_table_davide, time_table_ruoxi), 
        "machine" = c(rep("desktop", nrow(time_table_davide)),
                      rep("HPC cluster", nrow(time_table_ruoxi))))

time_table_hdf5_geom <- time_table_hdf5_geom %>% 
  dplyr::rename(chunk_size = geometry)  %>% 
  mutate(chunk_size = 
           factor(chunk_size, levels = c("best", "default",
                                         "worst", "singleChunk")), 
         machine = factor(machine, levels = c("desktop", "HPC cluster")))
levels(time_table_hdf5_geom$chunk_size) <- 
  c("per cell (best)", "default", "per gene (worst)", "single chunk")
```


### Simulated data

This currently loads in results from only Ruoxi. 
Need to add Davide's output (should be ready by 4/29).

#### Varying $k$

```{r}
files_vark_time <- list.files(path = here("output_tables", "Varying_k", "time"), 
                        pattern="*.csv", full.names=TRUE, recursive=FALSE)
vark_table_time <- NULL
for (i in (1:length(files_vark_time))){
  temp_table <- read.csv(file = files_vark_time[i], header=TRUE, sep=",")
  vark_table_time <- rbind(vark_table_time, temp_table)
}

vark_table_time <- vark_table_time %>% 
  dplyr::rename(Algorithm = method)

vark_table_time$Algorithm <- factor(vark_table_time$Algorithm, 
                               levels = c("kmeans", "mbkmeans", "hdf5"))
levels(vark_table_time$Algorithm) <- c("k-means", "mbkmeans", "mbkmeans (HDF5)")
```


## Accuracy

Here we are using the performance metrics adjusted Rand index (ARI) and 
within-clusters sum of squares (WCSS). 
All assessments in this section use absolute batch sizes.

### Simulated data

#### Fixed $k$

This loads in results from both Davide and Ruoxi. 

```{r}
files_acc <- list.files(path = here("output_tables", "abs_batch", "acc"), 
                        pattern="*.csv", full.names=TRUE, recursive=FALSE)

files_acc_davide <- files_acc[grep("_davide_mac", files_acc)]
files_acc_ruoxi <- files_acc[grep("_ruoxi_cluster", files_acc)]

acc_table_abs_davide <- NULL
for (i in (1:length(files_acc_davide))){
  temp_table <- read.csv(file = files_acc_davide[i], header=TRUE, sep=",")
  acc_table_abs_davide <- rbind(acc_table_abs_davide, temp_table)
}


acc_table_abs_ruoxi <- NULL
for (i in (1:length(files_acc_ruoxi))){
  temp_table <- read.csv(file = files_acc_ruoxi[i], header=TRUE, sep=",")
  acc_table_abs_ruoxi <- rbind(acc_table_abs_ruoxi, temp_table)
}

acc_table_abs <- 
  cbind(rbind(acc_table_abs_davide, acc_table_abs_ruoxi), 
      "machine" = c(rep("desktop", nrow(acc_table_abs_davide)),
                    rep("HPC cluster", nrow(acc_table_abs_ruoxi))))

acc_table_abs <- acc_table_abs %>% 
  transform(normal_WCSS = WCSS/observations) %>% 
  dplyr::rename(Algorithm = method)
acc_table_abs$machine <- factor(acc_table_abs$machine, 
                                levels = c("desktop", "HPC cluster"))
acc_table_abs$Algorithm <- factor(acc_table_abs$Algorithm, 
                               levels = c("kmeans", "mbkmeans", "hdf5"))
levels(acc_table_abs$Algorithm) <- c("k-means", "mbkmeans", "mbkmeans (HDF5)")
```


#### Varying $k$

This currently loads in results from only Ruoxi. 
Need to add Davide's output (should be ready by 4/29).

```{r}
files_vark <- list.files(path = here("output_tables", "Varying_k", "acc"), 
                        pattern="*.csv", full.names=TRUE, recursive=FALSE)
vark_table_abs <- NULL
for (i in (1:length(files_vark))){
  temp_table <- read.csv(file = files_vark[i], header=TRUE, sep=",")
  vark_table_abs <- rbind(vark_table_abs, temp_table)
}

vark_table_abs <- vark_table_abs %>% 
  transform(normal_WCSS = WCSS/observations) %>% 
  dplyr::rename(Algorithm = method)
vark_table_abs$Algorithm <- factor(vark_table_abs$Algorithm, 
                               levels = c("kmeans", "mbkmeans", "hdf5"))
levels(vark_table_abs$Algorithm) <- c("k-means", "mbkmeans", "mbkmeans (HDF5)")
```

### Subsampling TENxBrainData 


#### Fixed $k$

This currently loads in results from only Ruoxi. 
Need to add Davide's output. 

```{r}
# load in Ruoxi's files
files_acc_ruoxi <- list.files(path = here("output_tables", "abs_batch", 
                                    "acc", "real_data"), 
                        pattern="*.csv", full.names=TRUE, recursive=FALSE)
files_acc_ruoxi <- files_acc_ruoxi[grep("_ruoxi_cluster", files_acc_ruoxi)]

acc_real_table_abs_ruoxi <- NULL
for (i in (1:length(files_acc_ruoxi))){
  temp_table <- read.csv(file = files_acc_ruoxi[i], header=TRUE, sep=",")
  acc_real_table_abs_ruoxi <- rbind(acc_real_table_abs_ruoxi, temp_table)
}
head(acc_real_table_abs_ruoxi) 
# > table(acc_real_table_abs_ruoxi$observations)
#   5000 10000 25000 
#   1050  1050  1050 

# load in Davide's files
files_acc_davide <- list.files(path = here("main", "case_studies", "output"), 
                        pattern="*.csv", full.names=TRUE, recursive=FALSE)

files_acc_davide <- files_acc_davide[grep("wcss_TENxBrainData", files_acc_davide)]
files_acc_davide <- files_acc_davide[grep("_davide_mac", files_acc_davide)]

acc_real_table_abs_davide <- NULL
for (i in (1:length(files_acc_davide))){
  temp_table <- read.csv(file = files_acc_davide[i], header=FALSE, sep=",")
  acc_real_table_abs_davide <- rbind(acc_real_table_abs_davide, temp_table)
}
head(acc_real_table_abs_davide)
# > table(acc_real_table_abs_davide$V3)
#  75000  150000  300000  500000  750000 1000000 
#     11      11      11      10      10      10 
acc_real_table_abs_davide <- 
  acc_real_table_abs_davide[,-c(1:2)][,c(6,1,2,5,4,7)]
colnames(acc_real_table_abs_davide) <- colnames(acc_real_table_abs_ruoxi)[-5]
acc_real_table_abs_davide <- 
  acc_real_table_abs_davide %>% 
  mutate(abs_batch = round(observations*abs_batch))


acc_real_table_abs <- 
  cbind(rbind(acc_real_table_abs_davide, acc_real_table_abs_ruoxi[-5]), 
      "machine" = c(rep("desktop", nrow(acc_real_table_abs_davide)),
                    rep("HPC cluster", nrow(acc_real_table_abs_ruoxi))))

acc_real_table_abs <- acc_real_table_abs %>% 
  transform(normal_WCSS = WCSS/observations) %>% 
  dplyr::rename(Algorithm = method)
acc_table_abs$machine <- factor(acc_table_abs$machine, 
                                levels = c("desktop", "HPC cluster"))
acc_real_table_abs$Algorithm <- factor(acc_real_table_abs$Algorithm, 
                               levels = c("kmeans", "mbkmeans", "hdf5"))
levels(acc_real_table_abs$Algorithm) <- c("k-means", "mbkmeans", "mbkmeans (HDF5)")
# > table(acc_real_table_abs$machine, acc_real_table_abs$abs_batch)
#                   
#                     10  35  75 150 300 500 750 1000 1500 3000 5000 7500 10000
#  HPC cluster   450 450 450 450   0 450 450  450    0    0    0    0     0
#  desktop   0   0   6   6   5   6   1    6    7    8    6    6     6
```




# Figures 

## Figure 1

Here, we downsampling from the `TENxBrainData` 
($N=$ 75000, 150000, 300000, 500000  750000, and 1000000 observations)
and show the impact of memory and time with `mbkmeans` and 
k-means using a specific $k=15$ and batch size (`batch` = 500). 
In this figure we show the desktop 
results and show the HPC cluster results in supplemental. 

### Memory

```{r}
fig1_mem <- sub_mem_table %>% 
  dplyr::filter(Algorithm == "k-means" | batch == 500, 
                machine == "desktop") %>% 
  dplyr::select(!c(Dataset, machine, step, batch_size, B)) %>% 
  dplyr::select(Algorithm, everything()) %>% 
  dplyr::arrange(Algorithm, ncells)
fig1_mem[fig1_mem$Algorithm == "k-means",]$batch <- NA
# readr::write_csv(fig1_mem, path = here("main", "csv-tables", "fig1-mem.csv"))
```

```{r}
sub_mem_plot <- sub_mem_table %>% 
  dplyr::filter(Algorithm == "k-means" | batch == 500, 
                machine == "desktop") %>%
  dplyr::group_by(Algorithm) %>% 
  ggplot(aes(x = ncells, y = memory, color = Algorithm)) +
          geom_line()+
          geom_point(size = 3)+
      labs(title = "Memory usage for increasing number of cells") + 
        xlab("Number of cells") + 
        ylab("Max memory usage (GB)") + 
  theme(legend.position = "top", legend.justification= "center")
sub_mem_plot
```

Save the legend on its own
```{r}
my_legend <- cowplot::get_legend(sub_mem_plot)
```

Resave figure without legend

```{r}
sub_mem_plot <- 
  sub_mem_plot + 
  theme(legend.position = "none", legend.justification= "center")
```

### Time

```{r}
fig1_time <- sub_time_table %>% 
  dplyr::filter(Algorithm == "k-means" | batch == 500, 
                machine == "desktop") %>% 
  dplyr::group_by(Algorithm, ncells) %>%
  dplyr::summarize(elapsed_mean = mean(elapsed/60), elapsed_sd = sd(elapsed/60))
# readr::write_csv(fig1_time, path = here("main", "csv-tables", "fig1-time.csv"))
```

```{r}
fig1_both <- dplyr::left_join(fig1_mem, fig1_time) %>% 
  dplyr::select(Algorithm, ncells, ngenes, batch, everything())
readr::write_csv(fig1_both, path = here("main", "csv-tables", "fig1-stats.csv"))
```


```{r}
sub_time_plot <- sub_time_table %>% 
  dplyr::filter(Algorithm == "k-means" | batch == 500, 
                machine == "desktop") %>%
  dplyr::group_by(Algorithm, ncells) %>%
  dplyr::summarize(mean_elapsed = mean(elapsed/60), sd = sd(elapsed/60)) %>%
  ggplot(aes(x = ncells, y = mean_elapsed, color = Algorithm)) +
          geom_line() +
          geom_point(size = 3) +
          geom_errorbar(aes(ymin=mean_elapsed-sd, ymax=mean_elapsed+sd), width=.2,
                        position=position_dodge(0.05)) +
      labs(title = "Elapsed time for increasing number of cells") + 
        xlab("Number of cells") + 
        ylab("Elapsed time (mins)") + 
  theme(legend.position = "none", legend.justification= "center") 
sub_time_plot
```

```{r, fig.align="center"}
p1 <- ggdraw() +
    draw_plot(as_ggplot(my_legend), x = 0, y = 0.96, width = 1, height = 0.05) + 
    draw_plot(sub_mem_plot, x = 0, y = 0.48, width = 1, height = .48) +
    draw_plot(sub_time_plot, x = 0, y = 0, width = 1, height = .48) +
    draw_plot_label(label = LETTERS[1:2], size = 25,
                    x = c(0, 0), y = c(0.98, 0.51))
    
p1

pdf(here::here("main", "figs", "fig-real-timeandmemory-bs500.pdf"), 
    width = 6, height =8)
print(p1)
dev.off()
```



## Figure 2

Here we select three sizes of datasets ($N$) and show the accuracy 
as a function of absolute batch size using both simulated and 
real scRNA-seq data. In this figure we show the desktop 
results and show the HPC cluster results in supplemental. 

### ARI 
```{r}
p_ari_obs_line_abs <- acc_table_abs %>% 
  dplyr::filter(observations %in% c(5000,10000,25000), 
                machine == "desktop") %>%
  dplyr::group_by(Algorithm, observations, abs_batch) %>% 
  dplyr::summarize(mean = mean(ARI), 
                   sd = sd(ARI)) %>% 
  ggplot(aes(x = abs_batch, y = mean, color = Algorithm)) +
          geom_line() +
          geom_point() +
          geom_errorbar(aes(ymin=mean-sd, ymax=mean+sd), width=.2,
                position=position_dodge(0.05)) +
      ylim(0.95, 1) + 
      labs(title = "Performance of accuracy with three simulated scRNA-seq datasets") + 
        xlab("Absolute batch sizes") + 
        ylab("ARI") + 
  theme(legend.position = "top", legend.justification= "center") +
  facet_wrap(~observations, scales = "free") 
p_ari_obs_line_abs
```

Save the legend on its own
```{r}
my_legend <- cowplot::get_legend(p_ari_obs_line_abs)
```

Resave figure without legend

```{r}
p_ari_obs_line_abs <- 
  p_ari_obs_line_abs + 
  theme(legend.position = "none", legend.justification= "center")
```



### WCSS
```{r}
p_wcss_obs_line_abs <- acc_table_abs %>% 
  dplyr::filter(observations %in% c(5000,10000,25000), 
                machine == "desktop") %>%
  dplyr::group_by(Algorithm, observations, abs_batch) %>% 
  mutate(q90 = quantile(normal_WCSS, 0.9), row = row_number()) %>% 
  dplyr::filter(normal_WCSS <= q90) %>%
  dplyr::summarize(mean = mean(normal_WCSS), 
                   sd = sd(normal_WCSS)) %>% 
  ggplot(aes(x = abs_batch, y = mean, color = Algorithm)) +
          geom_line() +
          geom_point() +
          geom_errorbar(aes(ymin=mean-sd, ymax=mean+sd), width=.2,
                position=position_dodge(0.05))+
     labs(title = "Performance of accuracy with three simulated scRNA-seq datasets") + 
        xlab("Absolute batch sizes") + 
        ylab("WCSS") + 
  theme(legend.position = "none", legend.justification= "center") +
  facet_wrap(~observations, scales = "free") 
p_wcss_obs_line_abs
```

!!! This need to be changed to `machine == "solid disk drive"` when obs match.
Need to ask Davide about this. 

```{r}

dat <- acc_real_table_abs %>%   
  dplyr::filter(machine == "desktop")

p_wcss_real_abs <- acc_real_table_abs %>%   
  dplyr::filter(machine == "HPC cluster") %>% 
  dplyr::filter((observations %in% c(5000,10000,25000)))%>%
  dplyr::group_by(observations, Algorithm, abs_batch) %>% 
  mutate(q90 = quantile(normal_WCSS, 0.8), row = row_number()) %>% 
  dplyr::filter(normal_WCSS <= q90) %>%
  dplyr::summarize(mean = median(normal_WCSS), 
                   sd = sd(normal_WCSS)) %>% 
  ggplot(aes(x = abs_batch, y = mean, color = Algorithm)) +
          geom_line()+
          geom_point()+
          geom_errorbar(aes(ymin=mean-sd, ymax=mean+sd), width=.2,
                position=position_dodge(0.05))+
  theme(legend.position = "none", legend.justification= "center") +
  labs(title = "Performance of accuracy with three real scRNA-seq datasets") + 
        xlab("Absolute batch sizes") + 
        ylab("WCSS") +
  facet_wrap(~observations, scales = "free") 
p_wcss_real_abs
```

### save fig

```{r}
p2_abs <- ggdraw() +
    draw_plot(as_ggplot(my_legend), x = 0, y = 0.94, width = 1, height = 0.05) + 
    draw_plot(p_ari_obs_line_abs, x = 0, y = 0.62, width = 1, height = .32) +
    draw_plot(p_wcss_obs_line_abs, x = 0, y = 0.31, width = 1, height = .31) +
    draw_plot(p_wcss_real_abs, x = 0, y = 0, width = 1, height = .31) +
    draw_plot_label(label = LETTERS[1:3], size = 25,
                    x = c(0, 0, 0), y = c(1, 0.64, 0.33))
    
p2_abs

pdf(here::here("main", "figs", "fig-sims-and-real-accuracy.pdf"), 
    width = 10, height =10)
print(p2_abs)
dev.off()
```


## Figure 3

Here we select one size of datasets ($N = 1,000,000$) and show role of 
batch size on time and memory using real scRNA-seq data. 
as a function of absolute batch size using both simulated and 
real scRNA-seq data. In this figure we show the desktop 
results and show the HPC cluster results in supplemental. 


### Memory

```{r}
fig3_mem <- sub_mem_table %>% 
  dplyr::filter(ncells == 1000000, 
                Algorithm != "k-means",
                machine == "desktop") %>%
  dplyr::group_by(Algorithm, ncells, batch) %>%
  dplyr::select(!c(Dataset, machine, step, batch_size, B)) %>% 
  dplyr::select(Algorithm, everything()) %>% 
  dplyr::arrange(Algorithm, batch)
# readr::write_csv(fig3_mem, path = here("main", "csv-tables", "fig3-mem.csv"))
```

```{r}
fig3_mem_plot <- sub_mem_table %>% 
  dplyr::filter(ncells == 1000000, 
                Algorithm != "k-means",
                machine == "desktop") %>%
  dplyr::group_by(Algorithm, ncells, batch) %>%
  ggplot(aes(x = batch, y = memory, color = Algorithm)) +
          geom_line() +
          geom_point(size = 3) +
      labs(title = "Memory usage for increasing batch size") + 
        xlab("Number of cells in batch") + 
        ylab("Max memory usage (GB)") + scale_x_log10() +
  theme(legend.position = "top", legend.justification= "center")
fig3_mem_plot
```


Save the legend on its own
```{r}
my_legend <- cowplot::get_legend(fig3_mem_plot)
```

Resave figure without legend

```{r}
fig3_mem_plot <- 
  fig3_mem_plot + 
  theme(legend.position = "none", legend.justification= "center")
```

### Time


```{r}
fig3_time <- sub_time_table %>% 
  dplyr::filter(ncells == 1000000, Algorithm != "k-means",
                machine == "desktop") %>% 
  dplyr::group_by(Algorithm, ncells, batch) %>%
  dplyr::summarize(elapsed_mean = mean(elapsed/60), elapsed_sd = sd(elapsed/60))
# readr::write_csv(fig3_time, path = here("main", "csv-tables", "fig3-time.csv"))
```

```{r}
fig3_both <- dplyr::left_join(fig3_mem, fig3_time) %>% 
  dplyr::select(Algorithm, ncells, ngenes, batch, everything())
readr::write_csv(fig3_both, path = here("main", "csv-tables", "fig3-stats.csv"))
```

```{r}
fig3_time_plot <- sub_time_table %>% 
  dplyr::filter(ncells == 1000000, Algorithm != "k-means",
                machine == "desktop") %>%
  dplyr::group_by(Algorithm, ncells, batch) %>%
  dplyr::summarize(mean_elapsed = mean(elapsed/60), sd = sd(elapsed/60)) %>%
  ggplot(aes(x = batch, y = mean_elapsed, color = Algorithm)) + 
          geom_line() +
          geom_point(size = 3) +
            geom_errorbar(aes(ymin=mean_elapsed-sd, ymax=mean_elapsed+sd), width=.2,
                position=position_dodge(0.05)) +
      labs(title = "Elapsed time for increasing batch size") + 
        xlab("Number of cells in batch") + 
        ylab("Elapsed time (mins)") + scale_x_log10() +
  theme(legend.position = "none", legend.justification= "center") 
fig3_time_plot
```


```{r}
p3 <- ggdraw() +
    draw_plot(as_ggplot(my_legend), x = 0, y = 0.96, width = 1, height = 0.05) + 
    draw_plot(fig3_mem_plot, x = 0, y = 0.48, width = 1, height = .48) +
    draw_plot(fig3_time_plot, x = 0, y = 0, width = 1, height = .48) +
    draw_plot_label(label = LETTERS[1:2], size = 25,
                    x = c(0, 0), y = c(0.98, 0.51))
p3

pdf(here::here("main", "figs", "fig-real-timeandmemory-acrossbatch.pdf"), width = 6, height =8)
print(p3)
dev.off()
```



## Figure 4

Here, we downsampling from the `TENxBrainData` 
($N=$ 75000, 150000, 300000, 500000  750000, and 1000000 observations)
and show the impact of memory and time with different HDF5 geometry file 
structures using a specific $k=15$ and batch size (`batch` = 500). 
In this figure we show the desktop results and show the HPC cluster 
results in supplemental. 

### Memory

```{r}
fig4_mem <- mem_table_hdf5_geom %>% 
  dplyr::filter(machine == "desktop") %>% 
  dplyr::select(!c(choice, label, machine)) %>% 
  dplyr::select(chunk_size, dimension_1, dimension_2, observations, genes, 
                abs.batch.size, memory) %>% 
  dplyr::rename(dim_1 = dimension_1, dim_2 = dimension_2, ncells = observations, 
                ngenes = genes, batch = abs.batch.size) %>%
  dplyr::arrange(chunk_size, ncells) %>% 
  dplyr::mutate(memory = memory/1024)
# readr::write_csv(fig4_mem, path = here("main", "csv-tables", "fig4-mem.csv"))
```

```{r}
p_mem_hdf5 <- mem_table_hdf5_geom %>% 
  dplyr::filter(machine == "desktop") %>% 
  dplyr::group_by(observations, chunk_size) %>% 
  ggplot(aes(x = observations, y = memory/1024, color=chunk_size)) +
          geom_point() +
          geom_line() +
    labs(title = "Memory usage when accessing HDF5 files\nwith different chunk sizes") + 
    xlab("Number of cells") +
    ylab("Max memory usage (GB)") + 
    geom_rect(aes(xmin = 50000, xmax = 100000, ymin = 6, ymax = 6.8),
               fill = "transparent", color = "#C77CFF", size = 1.5) + 
  scale_color_discrete(name = "Chunk size", guide = guide_legend(nrow = 2)) + 
  theme(legend.position = "top", legend.justification= "center") 
p_mem_hdf5
```

Save the legend on its own
```{r}
my_legend <- cowplot::get_legend(p_mem_hdf5)
```

Resave figure without legend

```{r}
p_mem_hdf5 <- p_mem_hdf5 + 
  theme(legend.position = "none", legend.justification= "center")
```


### Time

```{r}
fig4_time <- time_table_hdf5_geom %>% 
  dplyr::filter(machine == "desktop")  %>% 
  dplyr::select(!c(choice, label, machine, time2, time3)) %>% 
  dplyr::select(chunk_size, dimension_1, dimension_2, observations, genes, 
                abs.batch.size, time1) %>%
  dplyr::rename(dim_1 = dimension_1, dim_2 = dimension_2, ncells = observations, 
                ngenes = genes, batch = abs.batch.size) %>%
  dplyr::group_by(ncells, chunk_size) %>% 
  dplyr::summarize(elapsed_mean = mean(time1/60), elapsed_sd = sd(time1/60))
# readr::write_csv(fig4_time, path = here("main", "csv-tables", "fig4-time.csv"))
```

```{r}
fig4_both <- dplyr::left_join(fig4_mem, fig4_time) 
readr::write_csv(fig4_both, path = here("main", "csv-tables", "fig4-stats.csv"))
```


```{r}
p_time_hdf5 <- time_table_hdf5_geom %>% 
  dplyr::filter(machine == "desktop") %>%
  dplyr::group_by(observations, chunk_size) %>% 
  dplyr::summarize(mean = mean(time1/60), 
                   sd = sd(time1/60)) %>% 
  ggplot(aes(x = observations, y = mean, color = chunk_size)) +
          geom_point() +
          geom_line() +
          geom_errorbar(aes(ymin=mean-sd, ymax=mean+sd), width=.2,
                position=position_dodge(0.05)) +
    labs(title = "Elapsed time when accessing HDF5 files\nwith different chunk sizes") + 
    xlab("Number of cells") +
    ylab("Elapsed time (mins)") + 
    geom_rect(aes(xmin = 50000, xmax = 100000, ymin = 0, ymax = 4),
               fill = "transparent", color = "#C77CFF", size = 1.5) + 
    scale_color_discrete(name = "Chunk size") + 
    theme(legend.position = "none", legend.justification= "center") 
p_time_hdf5
```



```{r}
p4 <- ggdraw() +
    draw_plot(as_ggplot(my_legend), x = 0, y = 0.92, width = 1, height = 0.10) + 
    draw_plot(p_mem_hdf5, x = 0, y = 0.46, width = 1, height = .48) +
    draw_plot(p_time_hdf5, x = 0, y = 0, width = 1, height = .48) +
    draw_plot_label(label = LETTERS[1:2], size = 25,
                    x = c(0, 0), y = c(0.95, 0.51))
p4

pdf(here::here("main", "figs", "fig-real-hdf5-geometry.pdf"), width = 6, height =8)
print(p4)
dev.off()
```

## Figure 5

Add here. 

# Supplemental Figures

## Simulated data (all N)
All assessments in this section use absolute batch sizes.

### Memory 

#### Varying $k$

```{r}
p_vark_mem <- vark_table_mem %>% 
  dplyr::group_by(Algorithm, observations, batch_size, k) %>% 
  ggplot(aes(x = k, y = memory/1024, color = Algorithm)) +
          geom_line() +
          geom_point() +
          #geom_errorbar(aes(ymin=mean-sd, ymax=mean+sd), width=.2,
          #      position=position_dodge(0.05)) +
     labs(title = "Memory usage (RAM) with two sizes of simulated scRNA-seq datasets\nand three absolute batch sizes (75, 500, 1000) \n with the true number of clusters as k = 15") + 
        xlab("Number of clusters (k) used in clustering algorithm") + 
        ylab("Max memory usage (GB)") + 
  theme(legend.position = "top", legend.justification= "center") +
  facet_grid(batch_size ~ observations, scales = "free") 
p_vark_mem
```

### Time 

#### Varying $k$

```{r}
p_vark_time <- vark_table_time %>% 
  dplyr::group_by(Algorithm, observations, batch_size, k) %>% 
  mutate(q90 = quantile(elapsed_time, 0.85), row = row_number()) %>% 
  #mutate(q15 = quantile(elapsed_time, 0.15), row = row_number()) %>% 
  dplyr::filter(elapsed_time < q90) %>%
  dplyr::summarize(mean = mean(elapsed_time/60), 
                   sd = sd(elapsed_time/60)) %>%
  ggplot(aes(x = k, y = mean, color = Algorithm)) +
          geom_line() +
          geom_point() +
          geom_errorbar(aes(ymin=mean-sd, ymax=mean+sd), width=.2,
                position=position_dodge(0.05)) +
     labs(title = "Elapsed time with two sizes of simulated scRNA-seq datasets\nand three absolute batch sizes (75, 500, 1000) \n with the true number of clusters as k = 15") + 
        xlab("Number of clusters (k) used in clustering algorithm") + 
        ylab("Elapsed time (mins)") + 
  theme(legend.position = "top", legend.justification= "center") +
  facet_grid(batch_size ~ observations, scales = "free") 
p_vark_time
```


### Accuracy (ARI)

#### Fixed $k$

```{r}
p_ari_obs_line_abs_all <- acc_table_abs %>% 
  dplyr::filter(machine == "desktop") %>% 
  dplyr::group_by(Algorithm, observations, abs_batch) %>% 
  dplyr::summarize(mean = mean(ARI), 
                   sd = sd(ARI)) %>% 
  ggplot(aes(x = abs_batch, y = mean, color = Algorithm)) +
          geom_line() +
          geom_point() +
          geom_errorbar(aes(ymin=mean-sd, ymax=mean+sd), width=.2,
                position=position_dodge(0.05))+
      ylim(0.95, 1) + 
      labs(title = "Performance of accuracy with increasing sizes of simulated scRNA-seq\ndatasets using a desktop") + 
        xlab("Absolute batch sizes") + 
        ylab("adjusted Rand index (ARI)") + 
  theme(legend.position = "top", legend.justification= "center") +
  facet_wrap(~observations, scales = "free") 
p_ari_obs_line_abs_all
```

```{r}
pdf(here::here("main", "figs", "fig-supp-sim-ari-ssdrive.pdf"), width = 10, height =10)
print(p_ari_obs_line_abs_all)
dev.off()
```



```{r}
p_ari_obs_line_abs_all <- acc_table_abs %>% 
  dplyr::filter(machine == "HPC cluster") %>% 
  dplyr::group_by(Algorithm, observations, abs_batch) %>% 
  dplyr::summarize(mean = mean(ARI), 
                   sd = sd(ARI)) %>% 
  ggplot(aes(x = abs_batch, y = mean, color = Algorithm)) +
          geom_line() +
          geom_point() +
          geom_errorbar(aes(ymin=mean-sd, ymax=mean+sd), width=.2,
                position=position_dodge(0.05))+
      ylim(0.95, 1) + 
      labs(title = "Performance of accuracy with increasing sizes of simulated scRNA-seq\ndatasets using a HPC cluster") + 
        xlab("Absolute batch sizes") + 
        ylab("adjusted Rand index (ARI)") + 
  theme(legend.position = "top", legend.justification= "center") +
  facet_wrap(~observations, scales = "free") 
p_ari_obs_line_abs_all
```

```{r}
pdf(here::here("main", "figs", "fig-supp-sim-ari-hddrive.pdf"), width = 10, height =10)
print(p_ari_obs_line_abs_all)
dev.off()
```
#### Varying $k$ 

```{r}
p_vark_ari <- vark_table_abs %>% 
  dplyr::group_by(Algorithm, observations, batch_size, k) %>% 
  dplyr::summarize(mean = mean(ARI), 
                   sd = sd(ARI)) %>% 
  ggplot(aes(x = k, y = mean, color = Algorithm)) +
          geom_line() +
          geom_point() +
          geom_errorbar(aes(ymin=mean-sd, ymax=mean+sd), width=.2,
                position=position_dodge(0.05)) +
     labs(title = "Performance of accuracy with two sizes of simulated scRNA-seq datasets\nand three absolute batch sizes (75, 500, 1000) \n with the true number of clusters as k = 15") + 
        xlab("Number of clusters (k) used in clustering algorithm") + 
        ylab("adjusted Rand index (ARI)") + 
  theme(legend.position = "top", legend.justification= "center") +
  facet_grid(batch_size ~ observations, scales = "free") 
p_vark_ari
```

```{r}
pdf(here::here("main", "figs", "fig-supp-sim-ari-varyingk.pdf"), width = 10, height =10)
print(p_vark_ari)
dev.off()
```


### Accuracy (WCSS)

#### Fixed $k$

```{r}
p_wcss_obs_line_abs_all <- acc_table_abs %>% 
  dplyr::filter(machine == "desktop") %>% 
  dplyr::group_by(Algorithm, observations, abs_batch) %>% 
  mutate(q90 = quantile(normal_WCSS, 0.9), row = row_number()) %>% 
  dplyr::filter(normal_WCSS <= q90) %>%
  dplyr::summarize(mean = mean(normal_WCSS), 
                   sd = sd(normal_WCSS)) %>% 
  ggplot(aes(x = abs_batch, y = mean, color = Algorithm)) +
          geom_line() +
          geom_point() +
          geom_errorbar(aes(ymin=mean-sd, ymax=mean+sd), width=.2,
                position=position_dodge(0.05))+
     labs(title = "Performance of accuracy with increasing sizes of simulated scRNA-seq\ndatasets using a desktop") + 
        xlab("Absolute batch sizes") + 
        ylab("WCSS") + 
  theme(legend.position = "top", legend.justification= "center") +
  facet_wrap(~observations, scales = "free") 
p_wcss_obs_line_abs_all
```

```{r}
pdf(here::here("main", "figs", "fig-supp-sim-wcss-ssdrive.pdf"), width = 10, height =10)
print(p_wcss_obs_line_abs_all)
dev.off()
```

```{r}
p_wcss_obs_line_abs_all <- acc_table_abs %>% 
  dplyr::filter(machine == "HPC cluster") %>% 
  dplyr::group_by(Algorithm, observations, abs_batch) %>% 
  mutate(q90 = quantile(normal_WCSS, 0.9), row = row_number()) %>% 
  dplyr::filter(normal_WCSS <= q90) %>%
  dplyr::summarize(mean = mean(normal_WCSS), 
                   sd = sd(normal_WCSS)) %>% 
  ggplot(aes(x = abs_batch, y = mean, color = Algorithm)) +
          geom_line() +
          geom_point() +
          geom_errorbar(aes(ymin=mean-sd, ymax=mean+sd), width=.2,
                position=position_dodge(0.05))+
     labs(title = "Performance of accuracy with increasing sizes of simulated scRNA-seq\ndatasets using a HPC cluster") + 
        xlab("Absolute batch sizes") + 
        ylab("WCSS") + 
  theme(legend.position = "top", legend.justification= "center") +
  facet_wrap(~observations, scales = "free") 
p_wcss_obs_line_abs_all
```

```{r}
pdf(here::here("main", "figs", "fig-supp-sim-wcss-hddrive.pdf"), width = 10, height =10)
print(p_wcss_obs_line_abs_all)
dev.off()
```


#### Varying $k$ 

Using absolute batch sizes. Fixed $k$.
```{r}
p_vark_wcss <- vark_table_abs %>% 
  dplyr::group_by(Algorithm, observations, batch_size, k) %>% 
  dplyr::summarize(mean = mean(normal_WCSS), 
                   sd = sd(normal_WCSS)) %>% 
  ggplot(aes(x = k, y = mean, color = Algorithm)) +
          geom_line() +
          geom_point() +
          geom_errorbar(aes(ymin=mean-sd, ymax=mean+sd), width=.2,
                position=position_dodge(0.05))+
     labs(title = "Performance of accuracy with two sizes of simulated scRNA-seq datasets\nand three absolute batch sizes (75, 500, 1000)\n with the true number of clusters as k = 15") + 
        xlab("Number of clusters (k) used in clustering algorithm") + 
        ylab("WCSS") + 
  theme(legend.position = "top", legend.justification= "center") +
  facet_grid(batch_size ~ observations, scales = "free") 
p_vark_wcss
```

```{r}
pdf(here::here("main", "figs", "fig-supp-sim-wcss-varyingk.pdf"), width = 10, height =10)
print(p_vark_wcss)
dev.off()
```

## TENxBrainData

### Accuracy (WCSS)

Only three sizes of datasets were assessed (5000, 10000, 25000). These 
results were shown in Figure 2. 


### Memory

#### Increasing $N$

```{r}
sub_mem_plot <- sub_mem_table %>% 
  dplyr::filter(Algorithm == "k-means" | batch == 500) %>%
  dplyr::group_by(Algorithm, machine) %>% 
  ggplot(aes(x = ncells, y = memory, color = Algorithm)) +
          geom_line()+
          geom_point(size = 3, )+
      labs(title = "Memory usage for increasing number of cells") + 
        xlab("Number of cells") + 
        ylab("Max memory usage (GB)") + 
  theme(legend.position = "top", legend.justification= "center") + 
  facet_grid(~machine)
sub_mem_plot
```

```{r}
pdf(here::here("main", "figs", "fig-supp-real-memory-bs500-twoplatforms.pdf"), 
    width = 10, height =5)
print(sub_mem_plot)
dev.off()
```

##### Increasing $b$

```{r}
sub_mem_plot <- sub_mem_table %>% 
  dplyr::filter(ncells == 1000000, 
                Algorithm != "k-means") %>%
  dplyr::group_by(Algorithm, ncells, batch, machine) %>%
  dplyr::summarize(mean = mean(memory), sd = sd(memory)) %>%
  ggplot(aes(x = batch, y = mean, color = Algorithm)) +
          geom_line() +
          geom_point(size = 3) +
      labs(title = "Memory usage for increasing batch size") + 
        xlab("Number of cells in batch") + 
        ylab("Max memory usage (GB)") + scale_x_log10() +
  theme(legend.position = "top", legend.justification= "center") + 
  facet_grid(~machine)
sub_mem_plot
```

```{r}
pdf(here::here("main", "figs", "fig-supp-real-memory-acrossbatch-twoplatforms.pdf"), 
    width = 10, height =5)
print(sub_mem_plot)
dev.off()
```


#### HDF5 geometry

```{r}
p_mem_hdf5 <- mem_table_hdf5_geom %>% 
  dplyr::group_by(observations, chunk_size, machine) %>% 
  ggplot(aes(x = observations, y = memory/1024, color=chunk_size)) +
          geom_point() +
          geom_line() +
    labs(title = "Memory usage when accessing HDF5 files\nwith different chunk sizes") + 
    xlab("Number of cells") +
    ylab("Max memory usage (GB)") + 
  scale_color_discrete(name = "Chunk size") + 
  theme(legend.position = "top", legend.justification= "center") + 
  facet_grid(~machine)
p_mem_hdf5
```

```{r}
pdf(here::here("main", "figs", "fig-supp-real-memory-hdf5-geom-twoplatforms.pdf"), 
    width = 10, height =5)
print(p_mem_hdf5)
dev.off()
```




### Time

#### Increasing $N$

```{r}
sub_time_plot <- sub_time_table %>% 
  dplyr::filter(Algorithm == "k-means" | batch == 500) %>%
  dplyr::group_by(Algorithm, ncells, machine) %>% 
  dplyr::summarize(mean_elapsed = mean(elapsed/60), sd = sd(elapsed/60)) %>%
  ggplot(aes(x = ncells, y = mean_elapsed, color = Algorithm)) +
          geom_line() +
          geom_point(size = 3) +
          geom_errorbar(aes(ymin=mean_elapsed-sd, ymax=mean_elapsed+sd), width=.2,
                        position=position_dodge(0.05)) +
      labs(title = "Elapsed time for increasing number of cells") + 
        xlab("Number of cells") + 
        ylab("Elapsed time (mins)") + 
  theme(legend.position = "top", legend.justification= "center") + 
  facet_grid(~machine)
sub_time_plot
```

```{r}
pdf(here::here("main", "figs", "fig-supp-real-time-bs500-twoplatforms.pdf"), 
    width = 10, height =5)
print(sub_time_plot)
dev.off()
```


#### Increasing $b$ 

```{r}
sub_time_plot <- sub_time_table %>% 
  dplyr::filter(ncells == 1000000, 
                Algorithm != "k-means") %>%
  dplyr::group_by(Algorithm, ncells, batch, machine) %>%
  dplyr::summarize(mean = mean(elapsed/60), sd = sd(elapsed/60)) %>%
  ggplot(aes(x = batch, y = mean, color = Algorithm)) +
          geom_line() +
          geom_point(size = 3) +
            geom_errorbar(aes(ymin=mean-sd, ymax=mean+sd), width=.2,
                position=position_dodge(0.05)) +
      labs(title = "Elapsed time for increasing batch size") + 
        xlab("Number of cells in batch") + 
        ylab("Elapsed time (mins)") + scale_x_log10() +
  theme(legend.position = "top", legend.justification= "center") + 
  facet_grid(~machine)
sub_time_plot
```


```{r}
pdf(here::here("main", "figs", "fig-supp-real-time-acrossbatch-twoplatforms.pdf"), 
    width = 10, height =5)
print(sub_time_plot)
dev.off()
```


#### HDF5 geometry

```{r}
p_time_hdf5 <- time_table_hdf5_geom %>% 
  dplyr::group_by(observations, chunk_size, machine) %>% 
  dplyr::summarize(mean = mean(time1/60), 
                   sd = sd(time1/60)) %>% 
  ggplot(aes(x = observations, y = mean, color = chunk_size)) +
          geom_point() +
          geom_line() +
          geom_errorbar(aes(ymin=mean-sd, ymax=mean+sd), width=.2,
                position=position_dodge(0.05)) +
    labs(title = "Elapsed time when accessing HDF5 files\nwith different chunk sizes") + 
    xlab("Number of cells") +
    ylab("Elapsed time (mins)") + 
    scale_color_discrete(name = "Chunk size") + 
    theme(legend.position = "top", legend.justification= "center")  + 
   facet_grid(~machine)
p_time_hdf5
```


```{r}
pdf(here::here("main", "figs", "fig-supp-real-time-hdf5-geom-twoplatforms.pdf"), 
    width = 10, height =5)
print(p_time_hdf5)
dev.off()
```











```{r}
p_mem_ruoxi <- mem_table_ruoxi %>% 
  dplyr::group_by(observations, chunk_size) %>% 
  ggplot(aes(x = observations, y = memory/1024, color=chunk_size)) +
          geom_point() +
          geom_line() +
    labs(title = "Memory usage when accessing HDF5 files\nwith different chunk sizes") + 
    xlab("Number of cells") +
    ylab("Max memory usage (GB)") + 
    geom_rect(aes(xmin = 50000, xmax = 100000, ymin = 4.4, ymax = 5),
               fill = "transparent", color = "#C77CFF", size = 1.5) + 
  scale_color_discrete(name = "Chunk size", guide = guide_legend(nrow = 2)) + 
  theme(legend.position = "top", legend.justification= "center") 
p_mem_ruoxi
```

```{r}
p_time_ruoxi <- time_table_ruoxi %>% 
  dplyr::group_by(observations, chunk_size) %>% 
  dplyr::summarize(mean = mean(time1/60), 
                   sd = sd(time1/60)) %>% 
  ggplot(aes(x = observations, y = mean, color = chunk_size)) +
          geom_point()+
          geom_line()+
          geom_errorbar(aes(ymin=mean-sd, ymax=mean+sd), width=.2,
                position=position_dodge(0.05)) +
     labs(title = "Elapsed time when accessing HDF5 files\nwith different chunk sizes") + 
    xlab("Number of cells") +
    ylab("Elapsed time (mins)") + 
    geom_rect(aes(xmin = 50000, xmax = 100000, ymin = 0, ymax = 4),
               fill = "transparent", color = "#C77CFF", size = 1.5) + 
    scale_color_discrete(name = "Chunk size") + 
    theme(legend.position = "none", legend.justification= "center") 

p_time_ruoxi
```