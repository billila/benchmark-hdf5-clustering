---
title: mbkmeans manuscript figures
author: Ruoxi Liu, Stephanie Hicks, Davide Risso, Elizabeth Purdom
output: 
    html_document:
        theme: cosmo 
        toc: true
        toc_float: true
        highlight: tango
        number_sections: false
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE)
```

```{r, message=FALSE}
library(here)
library(tidyr)
library(dplyr)
library(ggplot2)
library(cowplot)
library(ggpubr)
library(gridExtra)
library(grid)
library(here)
theme_set(theme_cowplot())

if(!file.exists(here("main", "csv-tables"))){
  dir.create(here("main", "csv-tables"))
}
```

# Load data

## Memory

Here we are profiling memory useage in R using `Rprof()` in base R. 
All assessments in this section use absolute batch sizes.

### Subsampling TENxBrainData 

#### Increasing $b$ and $N$

```{r}
files_sub <- list.files(path= here("main", "case_studies", "output"), 
                        pattern="*.csv", full.names=TRUE, recursive=FALSE)

files_sub <- files_sub[grep("TENxBrainData", files_sub)]
files_sub <- files_sub[grep("k_ruoxi_cluster|k_davide_mac|k_stephanie_cluster", files_sub)]
files_sub_mem <- files_sub[grep("memory", files_sub)]

sub_mem_table <- NULL

for (i in (1:length(files_sub_mem))){
  temp_table <- read.csv(file = files_sub_mem[i], header=FALSE, sep=",")
  sub_mem_table <- rbind(sub_mem_table, temp_table)
}

colnames(sub_mem_table) <- c("Dataset", "machine", "ncells", "ngenes", "step",
                             "method", "batch_size", "B", "memory")
sub_mem_table <- sub_mem_table %>% 
  mutate(method = factor(sub_mem_table$method, levels = c("kmeans", "ClusterR", "mbkmeans", "hdf5")),
         batch = round(batch_size * ncells),
         memory = memory/1000, 
         machine = case_when(machine == "davide_mac" ~ "desktop", 
                             machine %in% c("ruoxi_cluster", 
                                            "stephanie_cluster") ~ "HPC cluster")) %>% 
  dplyr::rename(Algorithm = method)

levels(sub_mem_table$Algorithm) <- c("k-means", "ClusterR", "mbkmeans", "mbkmeans (HDF5)")
```

#### HDF5 geometry

```{r}
mem_table_davide <- 
  read.csv(here("ongoing_analysis", "ChunkTest", "TENxBrainData", 
                "Output", "mem_davide_mac.csv"), header=TRUE, sep=",")
mem_table_ruoxi <- 
  read.csv(here("ongoing_analysis", "ChunkTest", "TENxBrainData", 
                "Output", "mem_ruoxi_cluster.csv"), header=TRUE, sep=",")

mem_table_hdf5_geom <- 
  cbind(rbind(mem_table_davide, mem_table_ruoxi), 
        "machine" = c(rep("desktop", nrow(mem_table_davide)),
                    rep("HPC cluster", nrow(mem_table_ruoxi))))

mem_table_hdf5_geom <- mem_table_hdf5_geom %>% 
  dplyr::rename(chunk_size = geometry)  %>% 
  mutate(chunk_size = 
           factor(chunk_size, levels = c("best", "default",
                                         "worst", "singleChunk")), 
         machine = factor(machine, levels = c("desktop", "HPC cluster")))
levels(mem_table_hdf5_geom$chunk_size) <- 
  c("per cell (best)", "default", "per gene (worst)", "single chunk")
```

### Simulated data

#### Varying $k$

```{r}
files_vark_mem <- list.files(path = here("output_tables", "Varying_k", "mem"), 
                        pattern="*.csv", full.names=TRUE, recursive=FALSE)
files_vark_mem_davide <- files_vark_mem[grep("_davide_mac", files_vark_mem)]
files_vark_mem_ruoxi <- files_vark_mem[grep("_ruoxi_cluster", files_vark_mem)]

vark_table_mem_davide <- NULL
for (i in (1:length(files_vark_mem_davide))){
  temp_table <- read.csv(file = files_vark_mem_davide[i], header=TRUE, sep=",")
  vark_table_mem_davide <- rbind(vark_table_mem_davide, temp_table)
}

vark_table_mem_ruoxi <- NULL
for (i in (1:length(files_vark_mem_ruoxi))){
  temp_table <- read.csv(file = files_vark_mem_ruoxi[i], header=TRUE, sep=",")
  vark_table_mem_ruoxi <- rbind(vark_table_mem_ruoxi, temp_table)
}

vark_table_mem <- 
  cbind(rbind(vark_table_mem_davide, vark_table_mem_ruoxi), 
      "machine" = c(rep("desktop", nrow(vark_table_mem_davide)),
                    rep("HPC cluster", nrow(vark_table_mem_ruoxi))))

vark_table_mem <- vark_table_mem %>% 
  dplyr::rename(Algorithm = method)

vark_table_mem$machine <- factor(vark_table_mem$machine, 
                                levels = c("desktop", "HPC cluster"))
vark_table_mem$Algorithm <- factor(vark_table_mem$Algorithm, 
                               levels = c("kmeans", "mbkmeans", "hdf5"))
levels(vark_table_mem$Algorithm) <- c("k-means", "mbkmeans", "mbkmeans (HDF5)")
```

## Time 

### Subsampling TENxBrainData 

#### Increasing $b$ and $N$

```{r}
files_sub_time <- files_sub[grep("time", files_sub)]

sub_time_table <- NULL

for (i in (1:length(files_sub_time))){
  temp_table <- read.csv(file = files_sub_time[i], header=FALSE, sep=",")
  sub_time_table <- rbind(sub_time_table, temp_table)
}

colnames(sub_time_table) <- c("Dataset", "machine", "ncells", "ngenes", "step",
                             "method", "batch_size", "B", "user", "system",
                             "elapsed")
sub_time_table <- sub_time_table %>% 
  mutate(method = factor(method, levels = c("kmeans", "ClusterR", "mbkmeans", "hdf5")),
         batch = round(batch_size * ncells), 
         machine = case_when(machine == "davide_mac" ~ "desktop", 
                             machine %in% c("ruoxi_cluster", 
                                            "stephanie_cluster") ~ "HPC cluster")) %>% 
  dplyr::rename(Algorithm = method)

levels(sub_time_table$Algorithm) <- c("k-means", "ClusterR", "mbkmeans", "mbkmeans (HDF5)")
```


#### HDF5 geometry

```{r}
time_table_davide <- 
  read.csv(here("ongoing_analysis", "ChunkTest", "TENxBrainData",
                "Output", "time_davide_mac.csv"), header=TRUE, sep=",")
time_table_ruoxi <- 
  read.csv(here("ongoing_analysis", "ChunkTest", "TENxBrainData",
                "Output", "time_ruoxi_cluster.csv"), header=TRUE, sep=",")

time_table_hdf5_geom <- 
  cbind(rbind(time_table_davide, time_table_ruoxi), 
        "machine" = c(rep("desktop", nrow(time_table_davide)),
                      rep("HPC cluster", nrow(time_table_ruoxi))))

time_table_hdf5_geom <- time_table_hdf5_geom %>% 
  dplyr::rename(chunk_size = geometry)  %>% 
  mutate(chunk_size = 
           factor(chunk_size, levels = c("best", "default",
                                         "worst", "singleChunk")), 
         machine = factor(machine, levels = c("desktop", "HPC cluster")))
levels(time_table_hdf5_geom$chunk_size) <- 
  c("per cell (best)", "default", "per gene (worst)", "single chunk")
```


## Accuracy

Here we are using the performance metrics adjusted Rand index (ARI) and 
within-clusters sum of squares (WCSS). 
All assessments in this section use absolute batch sizes.

### Simulated data

#### Fixed $k$

```{r}
files_acc <- list.files(path = here("output_tables", "abs_batch", "acc"), 
                        pattern="*.csv", full.names=TRUE, recursive=FALSE)

files_acc_davide <- files_acc[grep("_davide_mac", files_acc)]
files_acc_ruoxi <- files_acc[grep("_ruoxi_cluster", files_acc)]

acc_table_abs_davide <- NULL
for (i in (1:length(files_acc_davide))){
  temp_table <- read.csv(file = files_acc_davide[i], header=TRUE, sep=",")
  acc_table_abs_davide <- rbind(acc_table_abs_davide, temp_table)
}


acc_table_abs_ruoxi <- NULL
for (i in (1:length(files_acc_ruoxi))){
  temp_table <- read.csv(file = files_acc_ruoxi[i], header=TRUE, sep=",")
  acc_table_abs_ruoxi <- rbind(acc_table_abs_ruoxi, temp_table)
}

acc_table_abs <- 
  cbind(rbind(acc_table_abs_davide, acc_table_abs_ruoxi), 
      "machine" = c(rep("desktop", nrow(acc_table_abs_davide)),
                    rep("HPC cluster", nrow(acc_table_abs_ruoxi))))

acc_table_abs <- acc_table_abs %>% 
  transform(normal_WCSS = WCSS/observations) %>% 
  dplyr::rename(Algorithm = method)
acc_table_abs$machine <- factor(acc_table_abs$machine, 
                                levels = c("desktop", "HPC cluster"))
acc_table_abs$Algorithm <- factor(acc_table_abs$Algorithm, 
                               levels = c("kmeans", "mbkmeans", "hdf5"))
levels(acc_table_abs$Algorithm) <- c("k-means", "mbkmeans", "mbkmeans (HDF5)")
```


#### Varying $k$

```{r}
files_vark <- list.files(path = here("output_tables", "Varying_k", "acc"), 
                        pattern="*.csv", full.names=TRUE, recursive=FALSE)
files_vark_davide <- files_vark[grep("_davide_mac", files_vark)]
files_vark_ruoxi <- files_vark[grep("_ruoxi_cluster", files_vark)]

vark_table_davide <- NULL
for (i in (1:length(files_vark_davide))){
  temp_table <- read.csv(file = files_vark_davide[i], header=TRUE, sep=",")
  vark_table_davide <- rbind(vark_table_davide, temp_table)
}

vark_table_ruoxi <- NULL
for (i in (1:length(files_vark_ruoxi))){
  temp_table <- read.csv(file = files_vark_ruoxi[i], header=TRUE, sep=",")
  vark_table_ruoxi <- rbind(vark_table_ruoxi, temp_table)
}

vark_table <- 
  cbind(rbind(vark_table_davide, vark_table_ruoxi), 
      "machine" = c(rep("desktop", nrow(vark_table_davide)),
                    rep("HPC cluster", nrow(vark_table_ruoxi))))

vark_table <- vark_table %>% 
  transform(normal_WCSS = WCSS/observations) %>% 
  dplyr::rename(Algorithm = method) 

vark_table$machine <- factor(vark_table$machine, 
                                levels = c("desktop", "HPC cluster"))
vark_table$Algorithm <- factor(vark_table$Algorithm, 
                               levels = c("kmeans", "mbkmeans", "hdf5"))
levels(vark_table$Algorithm) <- c("k-means", "mbkmeans", "mbkmeans (HDF5)")
```

### Subsampling TENxBrainData 


#### Fixed $k$

```{r}
files_acc <- list.files(path = here("output_tables", "abs_batch", "acc", "real_data"), 
                        pattern="*.csv", full.names=TRUE, recursive=FALSE)

files_acc_davide <- files_acc[grep("_davide_mac", files_acc)]
files_acc_hpc <- files_acc[grep("_ruoxi_cluster|_stephanie_cluster", files_acc)]

acc_real_table_abs_davide <- NULL
for (i in (1:length(files_acc_davide))){
  temp_table <- read.csv(file = files_acc_davide[i], header=TRUE, sep=",")
  acc_real_table_abs_davide <- rbind(acc_real_table_abs_davide, temp_table)
}

acc_real_table_abs_hpc <- NULL
for (i in (1:length(files_acc_hpc))){
  temp_table <- read.csv(file = files_acc_hpc[i], header=TRUE, sep=",")
  acc_real_table_abs_hpc <- rbind(acc_real_table_abs_hpc, temp_table)
}

acc_real_table_abs <- 
  cbind(rbind(acc_real_table_abs_davide, acc_real_table_abs_hpc), 
      "machine" = c(rep("desktop", nrow(acc_real_table_abs_davide)),
                    rep("HPC cluster", nrow(acc_real_table_abs_hpc))))

acc_real_table_abs <- 
  acc_real_table_abs %>% 
  transform(normal_WCSS = WCSS/observations) %>% 
  dplyr::rename(Algorithm = method)

acc_real_table_abs$machine <- factor(acc_real_table_abs$machine, 
                                levels = c("desktop", "HPC cluster"))
acc_real_table_abs$Algorithm <- factor(acc_real_table_abs$Algorithm, 
                               levels = c("kmeans", "ClusterR", "mbkmeans", "hdf5"))
levels(acc_real_table_abs$Algorithm) <- c("k-means", "ClusterR", "mbkmeans", "mbkmeans (HDF5)")
```




# Figures 

## Figure 1

Here, we downsampling from the `TENxBrainData` 
($N=$ 75000, 150000, 300000, 500000, 750000, and 1000000 observations)
and show the impact of memory and time with `mbkmeans` and 
k-means using a specific $k=15$ and batch size (`batch` = 500). 
In this figure we show the desktop 
results and show the HPC cluster results in supplemental. 

### Memory

```{r}
fig1_mem <- sub_mem_table %>% 
  dplyr::filter(Algorithm == "k-means" | batch == 500, 
                machine == "desktop") %>% 
  dplyr::select(!c(Dataset, machine, step, batch_size, B)) %>% 
  dplyr::select(Algorithm, everything()) %>% 
  dplyr::arrange(Algorithm, ncells)
fig1_mem[fig1_mem$Algorithm == "k-means",]$batch <- NA
# readr::write_csv(fig1_mem, path = here("main", "csv-tables", "fig1-mem.csv"))
```

```{r}
sub_mem_plot <- sub_mem_table %>% 
  dplyr::filter(Algorithm == "k-means" | batch == 500, 
                machine == "desktop") %>%
  dplyr::group_by(Algorithm) %>% 
  ggplot(aes(x = ncells, y = memory, color = Algorithm)) +
          geom_line()+
          geom_point(size = 3)+
      labs(title = "Memory usage for increasing number of cells") + 
        xlab("Number of cells") + 
        ylab("Max memory usage (GB)") + 
  theme(legend.position = "top", legend.justification= "center")
sub_mem_plot
```

Save the legend on its own
```{r}
my_legend <- cowplot::get_legend(sub_mem_plot)
```

Resave figure without legend

```{r}
sub_mem_plot <- 
  sub_mem_plot + 
  theme(legend.position = "none", legend.justification= "center")
```

### Time

```{r}
fig1_time <- sub_time_table %>% 
  dplyr::filter(Algorithm == "k-means" | batch == 500, 
                machine == "desktop") %>% 
  dplyr::group_by(Algorithm, ncells) %>%
  dplyr::summarize(elapsed_mean = mean(elapsed/60), elapsed_sd = sd(elapsed/60))
# readr::write_csv(fig1_time, path = here("main", "csv-tables", "fig1-time.csv"))
```

```{r}
fig1_both <- dplyr::left_join(fig1_mem, fig1_time) %>% 
  dplyr::select(Algorithm, ncells, ngenes, batch, everything())
readr::write_csv(fig1_both, path = here("main", "csv-tables", "fig1-stats.csv"))
```


```{r}
sub_time_plot <- sub_time_table %>% 
  dplyr::filter(Algorithm == "k-means" | batch == 500, 
                machine == "desktop") %>%
  dplyr::group_by(Algorithm, ncells) %>%
  dplyr::summarize(mean_elapsed = mean(elapsed/60), sd = sd(elapsed/60)) %>%
  ggplot(aes(x = ncells, y = mean_elapsed, color = Algorithm)) +
          geom_line() +
          geom_point(size = 3) +
          geom_errorbar(aes(ymin=mean_elapsed-sd, ymax=mean_elapsed+sd), width=.2,
                        position=position_dodge(0.05)) +
      labs(title = "Elapsed time for increasing number of cells") + 
        xlab("Number of cells") + 
        ylab("Elapsed time (mins)") + 
  theme(legend.position = "none", legend.justification= "center") 
sub_time_plot
```

```{r, fig.align="center"}
p1 <- ggdraw() +
    draw_plot(as_ggplot(my_legend), x = 0, y = 0.96, width = 1, height = 0.05) + 
    draw_plot(sub_mem_plot, x = 0, y = 0.48, width = 1, height = .48) +
    draw_plot(sub_time_plot, x = 0, y = 0, width = 1, height = .48) +
    draw_plot_label(label = LETTERS[1:2], size = 25,
                    x = c(0, 0), y = c(0.98, 0.51))
    
p1

pdf(here::here("main", "figs", "fig-real-timeandmemory-bs500.pdf"), 
    width = 6, height =8)
print(p1)
dev.off()
```



## Figure 2

Here we select three sizes of datasets ($N$) and show the accuracy 
as a function of absolute batch size using both simulated and 
real scRNA-seq data. In this figure we show the desktop 
results and show the HPC cluster results in supplemental. 

### ARI


```{r}
fig2_ari_sim <- acc_table_abs %>% 
  dplyr::filter(observations %in% c(5000,10000,25000), 
                machine == "desktop") %>%
  dplyr::group_by(Algorithm, observations, abs_batch) %>% 
  dplyr::summarize(mean = mean(ARI), 
                   sd = sd(ARI)) %>% 
  dplyr::rename(ncells = observations, batch = abs_batch, 
                ari_sim_mean = mean, ari_sim_sd = sd) %>% 
  dplyr::arrange(Algorithm, ncells, batch)
```

```{r}
p_ari_obs_line_abs <- acc_table_abs %>% 
  dplyr::filter(observations %in% c(5000,10000,25000), 
                machine == "desktop") %>%
  dplyr::group_by(Algorithm, observations, abs_batch) %>% 
  dplyr::summarize(mean = mean(ARI), 
                   sd = sd(ARI)) %>% 
  ggplot(aes(x = abs_batch, y = mean, color = Algorithm)) +
          geom_line() +
          geom_point() +
          geom_errorbar(aes(ymin=mean-sd, ymax=mean+sd), width=.2,
                position=position_dodge(0.05)) +
      ylim(0.95, 1) + 
      labs(title = "Performance of accuracy with three simulated scRNA-seq datasets") + 
        xlab("Absolute batch sizes") + 
        ylab("ARI") + 
  theme(legend.position = "top", legend.justification= "center") +
  facet_wrap(~observations, scales = "free") 
p_ari_obs_line_abs
```

Save the legend on its own
```{r}
my_legend <- cowplot::get_legend(p_ari_obs_line_abs)
```

Resave figure without legend

```{r}
p_ari_obs_line_abs <- 
  p_ari_obs_line_abs + 
  theme(legend.position = "none", legend.justification= "center")
```



### WCSS

```{r}
fig2_wcss_sim <- acc_table_abs %>% 
  dplyr::filter(observations %in% c(5000,10000,25000), 
                machine == "desktop") %>%
  dplyr::group_by(Algorithm, observations, abs_batch) %>% 
  mutate(q90 = quantile(normal_WCSS, 0.9), row = row_number()) %>% 
  dplyr::filter(normal_WCSS <= q90) %>%
  dplyr::summarize(mean = mean(normal_WCSS), 
                   sd = sd(normal_WCSS)) %>% 
  dplyr::rename(ncells = observations, batch = abs_batch, 
                wcss_sim_mean = mean, wcss_sim_sd = sd) %>% 
  dplyr::arrange(Algorithm, ncells, batch)
```

```{r}
p_wcss_obs_line_abs <- acc_table_abs %>% 
  dplyr::filter(observations %in% c(5000,10000,25000), 
                machine == "desktop") %>%
  dplyr::group_by(Algorithm, observations, abs_batch) %>% 
  mutate(q90 = quantile(normal_WCSS, 0.9), row = row_number()) %>% 
  dplyr::filter(normal_WCSS <= q90) %>%
  dplyr::summarize(mean = mean(normal_WCSS), 
                   sd = sd(normal_WCSS)) %>% 
  ggplot(aes(x = abs_batch, y = mean, color = Algorithm)) +
          geom_line() +
          geom_point() +
          geom_errorbar(aes(ymin=mean-sd, ymax=mean+sd), width=.2,
                position=position_dodge(0.05))+
     labs(title = "Performance of accuracy with three simulated scRNA-seq datasets") + 
        xlab("Absolute batch sizes") + 
        ylab("WCSS") + 
  theme(legend.position = "none", legend.justification= "center") +
  facet_wrap(~observations, scales = "free") 
p_wcss_obs_line_abs
```

```{r}
fig2_wcss_real <- acc_real_table_abs %>%   
  dplyr::filter(machine == "desktop") %>% 
  dplyr::filter((observations %in% c(5000,10000,25000))) %>%
  dplyr::group_by(observations, Algorithm, abs_batch) %>% 
  mutate(q90 = quantile(normal_WCSS, 0.9), row = row_number()) %>% 
  dplyr::filter(normal_WCSS <= q90) %>%
  dplyr::summarize(mean = median(normal_WCSS), 
                   sd = sd(normal_WCSS))  %>% 
  dplyr::rename(ncells = observations, batch = abs_batch, 
                wcss_real_mean = mean, wcss_real_sd = sd) %>% 
  dplyr::select(Algorithm, ncells, batch, everything()) 
```


```{r}
fig2_all <- dplyr::left_join(fig2_ari_sim, fig2_wcss_sim) %>% 
  dplyr::left_join(fig2_wcss_real)
readr::write_csv(fig2_all, path = here("main", "csv-tables", "fig2-stats.csv"))
```

```{r}
p_wcss_real_abs <- acc_real_table_abs %>%   
  dplyr::filter(machine == "desktop") %>% 
  dplyr::filter((observations %in% c(5000,10000,25000))) %>%
  dplyr::group_by(observations, Algorithm, abs_batch) %>% 
  mutate(q90 = quantile(normal_WCSS, 0.9), row = row_number()) %>% 
  dplyr::filter(normal_WCSS <= q90) %>%
  dplyr::summarize(mean = median(normal_WCSS), 
                   sd = sd(normal_WCSS)) %>% 
  ggplot(aes(x = abs_batch, y = mean, color = Algorithm)) +
          geom_line() +
          geom_point() +
          geom_errorbar(aes(ymin=mean-sd, ymax=mean+sd), width=.2,
                position=position_dodge(0.05)) +
  theme(legend.position = "none", legend.justification= "center") +
  labs(title = "Performance of accuracy with three real scRNA-seq datasets") + 
        xlab("Absolute batch sizes") + 
        ylab("WCSS") +
  facet_wrap( ~ observations, scales = "free") 
p_wcss_real_abs
```

### save fig

```{r}
p2_abs <- ggdraw() +
    draw_plot(as_ggplot(my_legend), x = 0, y = 0.94, width = 1, height = 0.05) + 
    draw_plot(p_ari_obs_line_abs, x = 0, y = 0.62, width = 1, height = .32) +
    draw_plot(p_wcss_obs_line_abs, x = 0, y = 0.31, width = 1, height = .31) +
    draw_plot(p_wcss_real_abs, x = 0, y = 0, width = 1, height = .31) +
    draw_plot_label(label = LETTERS[1:3], size = 25,
                    x = c(0, 0, 0), y = c(1, 0.64, 0.33))
    
p2_abs

pdf(here::here("main", "figs", "fig-sims-and-real-accuracy.pdf"), 
    width = 10, height =10)
print(p2_abs)
dev.off()
```


## Figure 3

Here we select one size of datasets ($N = 1,000,000$) and show role of 
batch size on time and memory using real scRNA-seq data. 
as a function of absolute batch size using both simulated and 
real scRNA-seq data. In this figure we show the desktop 
results and show the HPC cluster results in supplemental. 


### Memory

```{r}
fig3_mem <- sub_mem_table %>% 
  dplyr::filter(ncells == 1000000, 
                Algorithm != "k-means",
                machine == "desktop") %>%
  dplyr::group_by(Algorithm, ncells, batch) %>%
  dplyr::select(!c(Dataset, machine, step, batch_size, B)) %>% 
  dplyr::select(Algorithm, everything()) %>% 
  dplyr::arrange(Algorithm, batch)
# readr::write_csv(fig3_mem, path = here("main", "csv-tables", "fig3-mem.csv"))
```

```{r}
fig3_mem_plot <- sub_mem_table %>% 
  dplyr::filter(ncells == 1000000, 
                Algorithm != "k-means",
                machine == "desktop") %>%
  dplyr::group_by(Algorithm, ncells, batch) %>%
  ggplot(aes(x = batch, y = memory, color = Algorithm)) +
          geom_line() +
          geom_point(size = 3) +
      labs(title = "Memory usage for increasing batch size") + 
        xlab("Number of cells in batch") + 
        ylab("Max memory usage (GB)") + scale_x_log10() +
  theme(legend.position = "top", legend.justification= "center")
fig3_mem_plot
```


Save the legend on its own
```{r}
my_legend <- cowplot::get_legend(fig3_mem_plot)
```

Resave figure without legend

```{r}
fig3_mem_plot <- 
  fig3_mem_plot + 
  theme(legend.position = "none", legend.justification= "center")
```

### Time


```{r}
fig3_time <- sub_time_table %>% 
  dplyr::filter(ncells == 1000000, Algorithm != "k-means",
                machine == "desktop") %>% 
  dplyr::group_by(Algorithm, ncells, batch) %>%
  dplyr::summarize(elapsed_mean = mean(elapsed/60), elapsed_sd = sd(elapsed/60))
# readr::write_csv(fig3_time, path = here("main", "csv-tables", "fig3-time.csv"))
```

```{r}
fig3_both <- dplyr::left_join(fig3_mem, fig3_time) %>% 
  dplyr::select(Algorithm, ncells, ngenes, batch, everything())
readr::write_csv(fig3_both, path = here("main", "csv-tables", "fig3-stats.csv"))
```

```{r}
fig3_time_plot <- sub_time_table %>% 
  dplyr::filter(ncells == 1000000, Algorithm != "k-means",
                machine == "desktop") %>%
  dplyr::group_by(Algorithm, ncells, batch) %>%
  dplyr::summarize(mean_elapsed = mean(elapsed/60), sd = sd(elapsed/60)) %>%
  ggplot(aes(x = batch, y = mean_elapsed, color = Algorithm)) + 
          geom_line() +
          geom_point(size = 3) +
            geom_errorbar(aes(ymin=mean_elapsed-sd, ymax=mean_elapsed+sd), width=.2,
                position=position_dodge(0.05)) +
      labs(title = "Elapsed time for increasing batch size") + 
        xlab("Number of cells in batch") + 
        ylab("Elapsed time (mins)") + scale_x_log10() +
  theme(legend.position = "none", legend.justification= "center") 
fig3_time_plot
```


```{r}
p3 <- ggdraw() +
    draw_plot(as_ggplot(my_legend), x = 0, y = 0.96, width = 1, height = 0.05) + 
    draw_plot(fig3_mem_plot, x = 0, y = 0.48, width = 1, height = .48) +
    draw_plot(fig3_time_plot, x = 0, y = 0, width = 1, height = .48) +
    draw_plot_label(label = LETTERS[1:2], size = 25,
                    x = c(0, 0), y = c(0.98, 0.51))
p3

pdf(here::here("main", "figs", "fig-real-timeandmemory-acrossbatch.pdf"), width = 6, height =8)
print(p3)
dev.off()
```



## Figure 4

Here, we downsampling from the `TENxBrainData` 
($N=$ 75000, 150000, 300000, 500000  750000, and 1000000 observations)
and show the impact of memory and time with different HDF5 geometry file 
structures using a specific $k=15$ and batch size (`batch` = 500). 
In this figure we show the desktop results and show the HPC cluster 
results in supplemental. 

### Memory

```{r}
fig4_mem <- mem_table_hdf5_geom %>% 
  dplyr::filter(machine == "desktop") %>% 
  dplyr::select(!c(choice, label, machine)) %>% 
  dplyr::select(chunk_size, dimension_1, dimension_2, observations, genes, 
                abs.batch.size, memory) %>% 
  dplyr::rename(dim_1 = dimension_1, dim_2 = dimension_2, ncells = observations, 
                ngenes = genes, batch = abs.batch.size) %>%
  dplyr::arrange(chunk_size, ncells) %>% 
  dplyr::mutate(memory = memory/1024)
# readr::write_csv(fig4_mem, path = here("main", "csv-tables", "fig4-mem.csv"))
```

```{r}
p_mem_hdf5 <- mem_table_hdf5_geom %>% 
  dplyr::filter(machine == "desktop") %>% 
  dplyr::group_by(observations, chunk_size) %>% 
  ggplot(aes(x = observations, y = memory/1024, color=chunk_size)) +
          geom_point() +
          geom_line() +
    labs(title = "Memory usage when accessing HDF5 files\nwith different chunk sizes") + 
    xlab("Number of cells") +
    ylab("Max memory usage (GB)") + 
    geom_rect(aes(xmin = 50000, xmax = 100000, ymin = 6, ymax = 6.8),
               fill = "transparent", color = "#C77CFF", size = 1.5) + 
  scale_color_discrete(name = "Chunk size", guide = guide_legend(nrow = 2)) + 
  theme(legend.position = "top", legend.justification= "center") 
p_mem_hdf5
```

Save the legend on its own
```{r}
my_legend <- cowplot::get_legend(p_mem_hdf5)
```

Resave figure without legend

```{r}
p_mem_hdf5 <- p_mem_hdf5 + 
  theme(legend.position = "none", legend.justification= "center")
```


### Time

```{r}
fig4_time <- time_table_hdf5_geom %>% 
  dplyr::filter(machine == "desktop")  %>% 
  dplyr::select(!c(choice, label, machine, time2, time3)) %>% 
  dplyr::select(chunk_size, dimension_1, dimension_2, observations, genes, 
                abs.batch.size, time1) %>%
  dplyr::rename(dim_1 = dimension_1, dim_2 = dimension_2, ncells = observations, 
                ngenes = genes, batch = abs.batch.size) %>%
  dplyr::group_by(ncells, chunk_size) %>% 
  dplyr::summarize(elapsed_mean = mean(time1/60), elapsed_sd = sd(time1/60))
# readr::write_csv(fig4_time, path = here("main", "csv-tables", "fig4-time.csv"))
```

```{r}
fig4_both <- dplyr::left_join(fig4_mem, fig4_time) 
readr::write_csv(fig4_both, path = here("main", "csv-tables", "fig4-stats.csv"))
```


```{r}
p_time_hdf5 <- time_table_hdf5_geom %>% 
  dplyr::filter(machine == "desktop") %>%
  dplyr::group_by(observations, chunk_size) %>% 
  dplyr::summarize(mean = mean(time1/60), 
                   sd = sd(time1/60)) %>% 
  ggplot(aes(x = observations, y = mean, color = chunk_size)) +
          geom_point() +
          geom_line() +
          geom_errorbar(aes(ymin=mean-sd, ymax=mean+sd), width=.2,
                position=position_dodge(0.05)) +
    labs(title = "Elapsed time when accessing HDF5 files\nwith different chunk sizes") + 
    xlab("Number of cells") +
    ylab("Elapsed time (mins)") + 
    geom_rect(aes(xmin = 50000, xmax = 100000, ymin = 0, ymax = 4),
               fill = "transparent", color = "#C77CFF", size = 1.5) + 
    scale_color_discrete(name = "Chunk size") + 
    theme(legend.position = "none", legend.justification= "center") 
p_time_hdf5
```



```{r}
p4 <- ggdraw() +
    draw_plot(as_ggplot(my_legend), x = 0, y = 0.92, width = 1, height = 0.10) + 
    draw_plot(p_mem_hdf5, x = 0, y = 0.46, width = 1, height = .48) +
    draw_plot(p_time_hdf5, x = 0, y = 0, width = 1, height = .48) +
    draw_plot_label(label = LETTERS[1:2], size = 25,
                    x = c(0, 0), y = c(0.95, 0.51))
p4

pdf(here::here("main", "figs", "fig-real-hdf5-geometry.pdf"), width = 6, height =8)
print(p4)
dev.off()
```

## Figure 5

Add here. 

# Supplemental Figures

## Simulated data (all N)
All assessments in this section use absolute batch sizes.

### Memory 

#### Varying $k$

```{r}
p_vark_mem_desktop <- vark_table_mem %>% 
  dplyr::filter(machine == "desktop") %>% 
  dplyr::group_by(Algorithm, observations, batch_size, k) %>% 
  ggplot(aes(x = k, y = memory/1024, color = Algorithm)) +
          geom_line() +
          geom_point() +
     labs(title = "Memory usage (RAM) using a desktop") + 
        xlab("Number of clusters (k) used in clustering algorithm") + 
        ylab("Max memory usage (GB)") + 
  theme(legend.position = "top", legend.justification= "center") +
  facet_grid(batch_size ~ observations, scales = "free") 
p_vark_mem_desktop
```

Save the legend on its own
```{r}
my_legend <- cowplot::get_legend(p_vark_mem_desktop)
```

Resave figure without legend

```{r}
p_vark_mem_desktop <- 
  p_vark_mem_desktop + 
  theme(legend.position = "none", legend.justification= "center")
```

```{r}
p_vark_mem_hpc <- vark_table_mem %>% 
  dplyr::filter(machine == "HPC cluster") %>% 
  dplyr::group_by(Algorithm, observations, batch_size, k) %>% 
  ggplot(aes(x = k, y = memory/1024, color = Algorithm)) +
          geom_line() +
          geom_point() +
     labs(title = "Memory usage (RAM) using a HPC cluster") + 
        xlab("Number of clusters (k) used in clustering algorithm") + 
        ylab("Max memory usage (GB)") + 
  theme(legend.position = "none", legend.justification= "center") +
  facet_grid(batch_size ~ observations, scales = "free") 
p_vark_mem_hpc

```

```{r}
p4 <- ggdraw() +
    draw_plot(as_ggplot(my_legend), x = 0, y = 0.92, width = 1, height = 0.10) + 
    draw_plot(p_vark_mem_desktop, x = 0, y = 0.46, width = 1, height = .48) +
    draw_plot(p_vark_mem_hpc, x = 0, y = 0, width = 1, height = .48) +
    draw_plot_label(label = LETTERS[1:2], size = 25,
                    x = c(0, 0), y = c(0.95, 0.51))
p4

pdf(here::here("main", "figs", "fig-supp-sim-memory-varyingk.pdf"), width = 8, height =8)
print(p4)
dev.off()
```

### Accuracy (ARI)

#### Fixed $k$

```{r}
p_ari_obs_line_abs_all <- acc_table_abs %>% 
  dplyr::filter(machine == "desktop") %>% 
  dplyr::group_by(Algorithm, observations, abs_batch) %>% 
  dplyr::summarize(mean = mean(ARI), 
                   sd = sd(ARI)) %>% 
  ggplot(aes(x = abs_batch, y = mean, color = Algorithm)) +
          geom_line() +
          geom_point() +
          geom_errorbar(aes(ymin=mean-sd, ymax=mean+sd), width=.2,
                position=position_dodge(0.05))+
      ylim(0.95, 1) + 
      labs(title = "Performance of accuracy with increasing sizes of simulated scRNA-seq\ndatasets using a desktop") + 
        xlab("Absolute batch sizes") + 
        ylab("adjusted Rand index (ARI)") + 
  theme(legend.position = "top", legend.justification= "center") +
  facet_wrap(~observations, scales = "free") 
p_ari_obs_line_abs_all
```

```{r}
pdf(here::here("main", "figs", "fig-supp-sim-ari-desktop.pdf"), width = 10, height =10)
print(p_ari_obs_line_abs_all)
dev.off()
```

```{r}
p_ari_obs_line_abs_all <- acc_table_abs %>% 
  dplyr::filter(machine == "HPC cluster") %>% 
  dplyr::group_by(Algorithm, observations, abs_batch) %>% 
  dplyr::summarize(mean = mean(ARI), 
                   sd = sd(ARI)) %>% 
  ggplot(aes(x = abs_batch, y = mean, color = Algorithm)) +
          geom_line() +
          geom_point() +
          geom_errorbar(aes(ymin=mean-sd, ymax=mean+sd), width=.2,
                position=position_dodge(0.05))+
      ylim(0.95, 1) + 
      labs(title = "Performance of accuracy with increasing sizes of simulated scRNA-seq\ndatasets using a HPC cluster") + 
        xlab("Absolute batch sizes") + 
        ylab("adjusted Rand index (ARI)") + 
  theme(legend.position = "top", legend.justification= "center") +
  facet_wrap(~observations, scales = "free") 
p_ari_obs_line_abs_all
```

```{r}
pdf(here::here("main", "figs", "fig-supp-sim-ari-hpc.pdf"), width = 10, height =10)
print(p_ari_obs_line_abs_all)
dev.off()
```

#### Varying $k$ 

```{r}
p_vark_ari_desktop <- vark_table %>% 
  dplyr::filter(machine == "desktop") %>% 
  dplyr::group_by(Algorithm, observations, batch_size, k) %>% 
  dplyr::summarize(mean = mean(ARI), 
                   sd = sd(ARI)) %>% 
  ggplot(aes(x = k, y = mean, color = Algorithm)) +
          geom_line() +
          geom_point() +
          geom_errorbar(aes(ymin=mean-sd, ymax=mean+sd), width=.2,
                position=position_dodge(0.05)) +
     labs(title = "Performance accuracy using a desktop") + 
        ylab("adjusted Rand index (ARI)") + 
  theme(legend.position = "top", legend.justification= "center") +
  facet_grid(batch_size ~ observations, scales = "free") 
p_vark_ari_desktop
```

Save the legend on its own
```{r}
my_legend <- cowplot::get_legend(p_vark_ari_desktop)
```

Resave figure without legend

```{r}
p_vark_ari_desktop <- 
  p_vark_ari_desktop + 
  theme(legend.position = "none", legend.justification= "center")
```

```{r}
p_vark_ari_hpc <- vark_table %>% 
  dplyr::filter(machine == "HPC cluster") %>% 
  dplyr::group_by(Algorithm, observations, batch_size, k) %>% 
  dplyr::summarize(mean = mean(ARI), 
                   sd = sd(ARI)) %>% 
  ggplot(aes(x = k, y = mean, color = Algorithm)) +
          geom_line() +
          geom_point() +
          geom_errorbar(aes(ymin=mean-sd, ymax=mean+sd), width=.2,
                position=position_dodge(0.05)) +
     labs(title = "Performance accuracy using a HPC cluster") + 
        ylab("adjusted Rand index (ARI)") + 
  theme(legend.position = "none", legend.justification= "center") +
  facet_grid(batch_size ~ observations, scales = "free") 
p_vark_ari_hpc
```

```{r}
p4 <- ggdraw() +
    draw_plot(as_ggplot(my_legend), x = 0, y = 0.92, width = 1, height = 0.10) + 
    draw_plot(p_vark_ari_desktop, x = 0, y = 0.46, width = 1, height = .48) +
    draw_plot(p_vark_ari_hpc, x = 0, y = 0, width = 1, height = .48) +
    draw_plot_label(label = LETTERS[1:2], size = 25,
                    x = c(0, 0), y = c(0.95, 0.51))
p4

pdf(here::here("main", "figs", "fig-supp-sim-ari-varyingk.pdf"), width = 8, height =12)
print(p4)
dev.off()
```

```{r}
pdf(here::here("main", "figs", "fig-supp-sim-ari-desktop-varyingk.pdf"), width = 10, height =10)
print(p_vark_ari_desktop)
dev.off()
```


### Accuracy (WCSS)

#### Fixed $k$

```{r}
p_wcss_obs_line_abs_all <- acc_table_abs %>% 
  dplyr::filter(machine == "desktop") %>% 
  dplyr::group_by(Algorithm, observations, abs_batch) %>% 
  mutate(q90 = quantile(normal_WCSS, 0.9), row = row_number()) %>% 
  dplyr::filter(normal_WCSS <= q90) %>%
  dplyr::summarize(mean = mean(normal_WCSS), 
                   sd = sd(normal_WCSS)) %>% 
  ggplot(aes(x = abs_batch, y = mean, color = Algorithm)) +
          geom_line() +
          geom_point() +
          geom_errorbar(aes(ymin=mean-sd, ymax=mean+sd), width=.2,
                position=position_dodge(0.05))+
     labs(title = "Performance of accuracy with increasing sizes of simulated scRNA-seq\ndatasets using a desktop") + 
        xlab("Absolute batch sizes") + 
        ylab("WCSS") + 
  theme(legend.position = "top", legend.justification= "center") +
  facet_wrap(~observations, scales = "free") 
p_wcss_obs_line_abs_all
```

```{r}
pdf(here::here("main", "figs", "fig-supp-sim-wcss-desktop.pdf"), width = 10, height =10)
print(p_wcss_obs_line_abs_all)
dev.off()
```

```{r}
p_wcss_obs_line_abs_all <- acc_table_abs %>% 
  dplyr::filter(machine == "HPC cluster") %>% 
  dplyr::group_by(Algorithm, observations, abs_batch) %>% 
  mutate(q90 = quantile(normal_WCSS, 0.9), row = row_number()) %>% 
  dplyr::filter(normal_WCSS <= q90) %>%
  dplyr::summarize(mean = mean(normal_WCSS), 
                   sd = sd(normal_WCSS)) %>% 
  ggplot(aes(x = abs_batch, y = mean, color = Algorithm)) +
          geom_line() +
          geom_point() +
          geom_errorbar(aes(ymin=mean-sd, ymax=mean+sd), width=.2,
                position=position_dodge(0.05))+
     labs(title = "Performance of accuracy with increasing sizes of simulated scRNA-seq\ndatasets using a HPC cluster") + 
        xlab("Absolute batch sizes") + 
        ylab("WCSS") + 
  theme(legend.position = "top", legend.justification= "center") +
  facet_wrap(~observations, scales = "free") 
p_wcss_obs_line_abs_all
```

```{r}
pdf(here::here("main", "figs", "fig-supp-sim-wcss-hpc.pdf"), width = 10, height =10)
print(p_wcss_obs_line_abs_all)
dev.off()
```

```{r}
p_wcss_real_abs <- acc_real_table_abs %>%   
  dplyr::filter(machine == "HPC cluster") %>% 
  dplyr::filter((observations %in% c(5000,10000,25000))) %>%
  dplyr::group_by(observations, Algorithm, abs_batch) %>% 
  mutate(q90 = quantile(normal_WCSS, 0.9), row = row_number()) %>% 
  dplyr::filter(normal_WCSS <= q90) %>%
  dplyr::summarize(mean = median(normal_WCSS), 
                   sd = sd(normal_WCSS)) %>% 
  ggplot(aes(x = abs_batch, y = mean, color = Algorithm)) +
          geom_line() +
          geom_point() +
          geom_errorbar(aes(ymin=mean-sd, ymax=mean+sd), width=.2,
                position=position_dodge(0.05)) +
  theme(legend.position = "top", legend.justification= "center") +
  labs(title = "Performance of accuracy with three real scRNA-seq datasets") + 
        xlab("Absolute batch sizes") + 
        ylab("WCSS") +
  facet_wrap( ~ observations, scales = "free") 
p_wcss_real_abs
```


#### Varying $k$ 

```{r}
p_vark_wcss_desktop <- vark_table %>% 
  dplyr::filter(machine == "desktop") %>% 
  dplyr::group_by(Algorithm, observations, batch_size, k) %>% 
  dplyr::summarize(mean = mean(normal_WCSS), 
                   sd = sd(normal_WCSS)) %>% 
  ggplot(aes(x = k, y = mean, color = Algorithm)) +
          geom_line() +
          geom_point() +
          geom_errorbar(aes(ymin=mean-sd, ymax=mean+sd), width=.2,
                position=position_dodge(0.05)) +
     labs(title = "Performance accuracy using a desktop") + 
        ylab("WCSS") + 
  theme(legend.position = "top", legend.justification= "center") +
  facet_grid(batch_size ~ observations, scales = "free") 
p_vark_wcss_desktop
```

Save the legend on its own
```{r}
my_legend <- cowplot::get_legend(p_vark_wcss_desktop)
```

Resave figure without legend

```{r}
p_vark_wcss_desktop <- 
  p_vark_wcss_desktop + 
  theme(legend.position = "none", legend.justification= "center")
```

```{r}
p_vark_wcss_hpc <- vark_table %>% 
  dplyr::filter(machine == "HPC cluster") %>% 
  dplyr::group_by(Algorithm, observations, batch_size, k) %>% 
  dplyr::summarize(mean = mean(normal_WCSS), 
                   sd = sd(normal_WCSS)) %>% 
  ggplot(aes(x = k, y = mean, color = Algorithm)) +
          geom_line() +
          geom_point() +
          geom_errorbar(aes(ymin=mean-sd, ymax=mean+sd), width=.2,
                position=position_dodge(0.05)) +
     labs(title = "Performance accuracy using a HPC cluster") + 
        ylab("WCSS") + 
  theme(legend.position = "none", legend.justification= "center") +
  facet_grid(batch_size ~ observations, scales = "free") 
p_vark_wcss_hpc
```

```{r}
p4 <- ggdraw() +
    draw_plot(as_ggplot(my_legend), x = 0, y = 0.92, width = 1, height = 0.10) + 
    draw_plot(p_vark_wcss_desktop, x = 0, y = 0.46, width = 1, height = .48) +
    draw_plot(p_vark_wcss_hpc, x = 0, y = 0, width = 1, height = .48) +
    draw_plot_label(label = LETTERS[1:2], size = 25,
                    x = c(0, 0), y = c(0.95, 0.51))
p4

pdf(here::here("main", "figs", "fig-supp-sim-wcss-varyingk.pdf"), width = 8, height =12)
print(p4)
dev.off()
```


## TENxBrainData

### Accuracy (WCSS)

Only three sizes of data sets were assessed (5000, 10000, 25000), but here we results from `ClusterR`.

```{r}
p_wcss_real_abs <- acc_real_table_abs %>%   
  dplyr::filter((observations %in% c(5000,10000,25000))) %>%
  dplyr::group_by(observations, Algorithm, abs_batch) %>% 
  mutate(q90 = quantile(normal_WCSS, 0.9), row = row_number()) %>% 
  dplyr::filter(normal_WCSS <= q90) %>%
  group_by(observations, Algorithm, abs_batch, machine) %>%
  dplyr::summarize(mean = median(normal_WCSS), 
                   sd = sd(normal_WCSS)) %>% 
  ggplot(aes(x = abs_batch, y = mean, color = Algorithm)) +
          geom_line() +
          geom_point() +
          geom_errorbar(aes(ymin=mean-sd, ymax=mean+sd), width=.2,
                position=position_dodge(0.05)) +
  theme(legend.position = "top", legend.justification= "center") +
  labs(title = "Performance of accuracy with three real scRNA-seq datasets") + 
        xlab("Absolute batch sizes") + 
        ylab("WCSS") +
  facet_grid(machine ~ observations, scales = "free") 
p_wcss_real_abs
```
```{r}
pdf(here::here("main", "figs", "fig-supp-real-wcss-twoplatforms.pdf"), 
    width = 8, height =6)
print(p_wcss_real_abs)
dev.off()
```

### Memory

#### Increasing $N$

```{r}
sub_mem_plot <- sub_mem_table %>% 
  dplyr::filter(Algorithm == "k-means" | batch == 500) %>%
  dplyr::group_by(Algorithm, machine) %>% 
  ggplot(aes(x = ncells, y = memory, color = Algorithm)) +
          geom_line()+
          geom_point(size = 3, )+
      labs(title = "Memory usage for increasing number of cells") + 
        xlab("Number of cells") + 
        ylab("Max memory usage (GB)") + 
  theme(legend.position = "top", legend.justification= "center") + 
  facet_grid(~machine)
sub_mem_plot
```

```{r}
pdf(here::here("main", "figs", "fig-supp-real-memory-bs500-twoplatforms.pdf"), 
    width = 10, height =5)
print(sub_mem_plot)
dev.off()
```

##### Increasing $b$

```{r}
sub_mem_plot <- sub_mem_table %>% 
  dplyr::filter(ncells == 1000000, 
                Algorithm != "k-means") %>%
  dplyr::group_by(Algorithm, ncells, batch, machine) %>%
  dplyr::summarize(mean = mean(memory), sd = sd(memory)) %>%
  ggplot(aes(x = batch, y = mean, color = Algorithm)) +
          geom_line() +
          geom_point(size = 3) +
      labs(title = "Memory usage for increasing batch size") + 
        xlab("Number of cells in batch") + 
        ylab("Max memory usage (GB)") + scale_x_log10() +
  theme(legend.position = "top", legend.justification= "center") + 
  facet_grid(~machine)
sub_mem_plot
```

```{r}
pdf(here::here("main", "figs", "fig-supp-real-memory-acrossbatch-twoplatforms.pdf"), 
    width = 10, height =5)
print(sub_mem_plot)
dev.off()
```


#### HDF5 geometry

```{r}
p_mem_hdf5 <- mem_table_hdf5_geom %>% 
  dplyr::group_by(observations, chunk_size, machine) %>% 
  ggplot(aes(x = observations, y = memory/1024, color=chunk_size)) +
          geom_point() +
          geom_line() +
    labs(title = "Memory usage when accessing HDF5 files\nwith different chunk sizes") + 
    xlab("Number of cells") +
    ylab("Max memory usage (GB)") + 
  scale_color_discrete(name = "Chunk size") + 
  theme(legend.position = "top", legend.justification= "center") + 
  facet_grid(~machine)
p_mem_hdf5
```

```{r}
pdf(here::here("main", "figs", "fig-supp-real-memory-hdf5-geom-twoplatforms.pdf"), 
    width = 10, height =5)
print(p_mem_hdf5)
dev.off()
```




### Time

#### Increasing $N$

```{r, message=FALSE}
sub_time_plot <- sub_time_table %>% 
  dplyr::filter(Algorithm == "k-means" | batch == 500) %>%
  dplyr::group_by(Algorithm, ncells, machine) %>% 
  dplyr::summarize(mean_elapsed = mean(elapsed/60), sd = sd(elapsed/60)) %>%
  ggplot(aes(x = ncells, y = mean_elapsed, color = Algorithm)) +
          geom_line() +
          geom_point(size = 3) +
          geom_errorbar(aes(ymin=mean_elapsed-sd, ymax=mean_elapsed+sd), width=.2,
                        position=position_dodge(0.05)) +
      labs(title = "Elapsed time for increasing number of cells") + 
        xlab("Number of cells") + 
        ylab("Elapsed time (mins)") + 
  theme(legend.position = "top", legend.justification= "center") + 
  facet_grid(~machine)
sub_time_plot
```

```{r}
pdf(here::here("main", "figs", "fig-supp-real-time-bs500-twoplatforms.pdf"), 
    width = 10, height =5)
print(sub_time_plot)
dev.off()
```


#### Increasing $b$ 

```{r}
sub_time_plot <- sub_time_table %>% 
  dplyr::filter(ncells == 1000000, 
                Algorithm != "k-means") %>%
  dplyr::group_by(Algorithm, ncells, batch, machine) %>%
  dplyr::summarize(mean = mean(elapsed/60), sd = sd(elapsed/60)) %>%
  ggplot(aes(x = batch, y = mean, color = Algorithm)) +
          geom_line() +
          geom_point(size = 3) +
            geom_errorbar(aes(ymin=mean-sd, ymax=mean+sd), width=.2,
                position=position_dodge(0.05)) +
      labs(title = "Elapsed time for increasing batch size") + 
        xlab("Number of cells in batch") + 
        ylab("Elapsed time (mins)") + scale_x_log10() +
  theme(legend.position = "top", legend.justification= "center") + 
  facet_grid(~machine)
sub_time_plot
```


```{r}
pdf(here::here("main", "figs", "fig-supp-real-time-acrossbatch-twoplatforms.pdf"), 
    width = 10, height =5)
print(sub_time_plot)
dev.off()
```


#### HDF5 geometry

```{r}
p_time_hdf5 <- time_table_hdf5_geom %>% 
  dplyr::group_by(observations, chunk_size, machine) %>% 
  dplyr::summarize(mean = mean(time1/60), 
                   sd = sd(time1/60)) %>% 
  ggplot(aes(x = observations, y = mean, color = chunk_size)) +
          geom_point() +
          geom_line() +
          geom_errorbar(aes(ymin=mean-sd, ymax=mean+sd), width=.2,
                position=position_dodge(0.05)) +
    labs(title = "Elapsed time when accessing HDF5 files\nwith different chunk sizes") + 
    xlab("Number of cells") +
    ylab("Elapsed time (mins)") + 
    scale_color_discrete(name = "Chunk size") + 
    theme(legend.position = "top", legend.justification= "center")  + 
   facet_grid(~machine)
p_time_hdf5
```


```{r}
pdf(here::here("main", "figs", "fig-supp-real-time-hdf5-geom-twoplatforms.pdf"), 
    width = 10, height =5)
print(p_time_hdf5)
dev.off()
```











```{r}
p_mem_ruoxi <- mem_table_ruoxi %>% 
  dplyr::group_by(observations, chunk_size) %>% 
  ggplot(aes(x = observations, y = memory/1024, color=chunk_size)) +
          geom_point() +
          geom_line() +
    labs(title = "Memory usage when accessing HDF5 files\nwith different chunk sizes") + 
    xlab("Number of cells") +
    ylab("Max memory usage (GB)") + 
    geom_rect(aes(xmin = 50000, xmax = 100000, ymin = 4.4, ymax = 5),
               fill = "transparent", color = "#C77CFF", size = 1.5) + 
  scale_color_discrete(name = "Chunk size", guide = guide_legend(nrow = 2)) + 
  theme(legend.position = "top", legend.justification= "center") 
p_mem_ruoxi
```

```{r}
p_time_ruoxi <- time_table_ruoxi %>% 
  dplyr::group_by(observations, chunk_size) %>% 
  dplyr::summarize(mean = mean(time1/60), 
                   sd = sd(time1/60)) %>% 
  ggplot(aes(x = observations, y = mean, color = chunk_size)) +
          geom_point()+
          geom_line()+
          geom_errorbar(aes(ymin=mean-sd, ymax=mean+sd), width=.2,
                position=position_dodge(0.05)) +
     labs(title = "Elapsed time when accessing HDF5 files\nwith different chunk sizes") + 
    xlab("Number of cells") +
    ylab("Elapsed time (mins)") + 
    geom_rect(aes(xmin = 50000, xmax = 100000, ymin = 0, ymax = 4),
               fill = "transparent", color = "#C77CFF", size = 1.5) + 
    scale_color_discrete(name = "Chunk size") + 
    theme(legend.position = "none", legend.justification= "center") 

p_time_ruoxi
```