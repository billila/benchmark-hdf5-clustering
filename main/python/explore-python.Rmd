---
title: Running MiniBatchKMeans from Python on HDF5 files (1.3 million neurons from 10X)
author: Stephanie Hicks
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Download and load data in R (sanity check)

Create the necessary folder structure for the `.h5` dataset
```{r}
suppressMessages({
    library(here)
    library(HDF5Array)
    library(rhdf5)
    library(SingleCellExperiment)
})

if(!dir.exists(here("main/case_studies/data/h5_python"))){
      dir.create(here("main/case_studies/data/h5_python"))
}

h5_data_path <- here("main", "case_studies", "data", "h5_python", 
                  "1M_neurons_filtered_gene_bc_matrices_h5.h5")
```

Download 1.3 million neuron dataset in the `.h5` file format
```{r}
if(!file.exists(here("main", "case_studies", "data", "h5_python", 
                     "1M_neurons_filtered_gene_bc_matrices_h5.h5"))){
  h5_file <- "http://cf.10xgenomics.com/samples/cell-exp/1.3.0/1M_neurons/1M_neurons_filtered_gene_bc_matrices_h5.h5"
  download.file(h5_file, destfile = h5_data_path, method = "wget")
}
```

We can see what is in our `.h5` file using the `rhdf5::h5ls()` function

```{r}
rhdf5::h5ls(h5_data_path)
# > rhdf5::h5ls(h5_data_path)
#   group       name       otype  dclass        dim
# 0     /       mm10   H5I_GROUP
# 1 /mm10   barcodes H5I_DATASET  STRING    1306127
# 2 /mm10       data H5I_DATASET INTEGER 2624828308
# 3 /mm10 gene_names H5I_DATASET  STRING      27998
# 4 /mm10      genes H5I_DATASET  STRING      27998
# 5 /mm10    indices H5I_DATASET INTEGER 2624828308
# 6 /mm10     indptr H5I_DATASET INTEGER    1306128
# 7 /mm10      shape H5I_DATASET INTEGER          
```

Try reading in the HDF5 file using the `HDF5Array` package as a sanity check
 # the function `HDF5Array::TENxMatrix()` is used for the HDF5 sparse representation
```{r}
h5_file <- TENxMatrix(h5_data_path)
# > h5_file
# <27998 x 1306127> sparse matrix of class TENxMatrix and type "integer":
#                      AAACCTGAGATAGGAG-1 ... TTTGTCATCTGAAAGA-133
# ENSMUSG00000051951                    0   .                    0
# ENSMUSG00000089699                    0   .                    0
# ENSMUSG00000102343                    0   .                    0
# ENSMUSG00000025900                    0   .                    0
# ENSMUSG00000109048                    0   .                    0
#                ...                    .   .                    .
# ENSMUSG00000079808                    0   .                    0
# ENSMUSG00000095041                    1   .                    0
# ENSMUSG00000063897                    0   .                    0
# ENSMUSG00000096730                    0   .                    0
# ENSMUSG00000095742                    0   .                    0
```

```{r}
pryr::object_size(h5_file)
# 133 MB
```

We will also write a dense HDF5 file in the most optimal way 
(i.e. chunked by cell) that well use later on: 

```{r}
h5_file_sub <- t(h5_file[,1:500])
h5_file_out <- writeHDF5Array(x=h5_file_sub, 
               filepath = here("main", "case_studies", "data", 
                               "h5_python", "1M_neurons_transposed_chunked.h5"), 
               name = "counts", chunkdim = c(1,dim(h5_file)[1]))
```

```{r}
tmp <- HDF5Array(here("main", "case_studies", "data", 
                      "h5_python", "1M_neurons_transposed_chunked.h5"),
                 name = "counts")
```



## Python 

### Getting Stephanie set up 

This section is really only need for Stephanie to work on her cluster. 
Here I am using Python 3. 
I had to update the numpy modules and install the scipy and pandas modules

```{bash}
module load conda_R/devel
python3 -m pip install h5py --user
python3 -m pip install tables --user
```

Next, I load the `reticulate` package

```{r}
library(reticulate)
use_python("/jhpce/shared/jhpce/core/conda/miniconda3-4.6.14/envs/svnR-devel/bin/python3")
repl_python()
```

Use `repl_python()` python repl from the R command. Use `exit` to exit the repl.
**Note**: Objects do not have permenancy in R after exiting the python repl.

```{python}
import numpy as np
import pandas as pd
import sys
import time

# kmeans, MiniBatchKMeans
from sklearn.cluster import KMeans, MiniBatchKMeans
```


So there are a couple of ways to read this data into Python.

#### Following CellRanger's recommendation: loads a compressed, sparse matrix

I started by looking at CellRanger's website and found these [instructions](https://support.10xgenomics.com/single-cell-gene-expression/software/pipelines/latest/advanced/h5_matrices). 
I copied (and had to do small modifications of) the code and placed below. 
The `sp_sparse.csc_matrix()` from the `scipy.sparse` Python module [reads in](https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csc_matrix.html) what's called a "Compressed Sparse Column matrix". 
Load data with specific cellranger code (needed to read in the sparse HDF5 cell ranger format)

```{python}
import collections
import scipy.sparse as sp_sparse
import tables

CountMatrix = collections.namedtuple('CountMatrix', ['feature_ref', 'barcodes', 'matrix'])
 
def get_matrix_from_h5(filename):
    with tables.open_file(filename, 'r') as f:
        mat_group = f.get_node(f.root, 'mm10')
        barcodes = f.get_node(mat_group, 'barcodes').read()
        data = getattr(mat_group, 'data').read()
        indices = getattr(mat_group, 'indices').read()
        indptr = getattr(mat_group, 'indptr').read()
        shape = getattr(mat_group, 'shape').read()
        matrix = sp_sparse.csc_matrix((data, indices, indptr), shape=shape)
        feature_ref = {}
        gene_ids = getattr(mat_group, 'genes').read()
        gene_names = getattr(mat_group, 'gene_names').read()
        feature_ref['id'] = gene_ids
        feature_ref['name'] = gene_names
         
        return CountMatrix(feature_ref, barcodes, matrix)


filtered_matrix_h5 = "/fastscratch/myscratch/shicks1/benchmark-hdf5-clustering/main/case_studies/data/h5_python/1M_neurons_filtered_gene_bc_matrices_h5.h5"
filtered_feature_bc_matrix = get_matrix_from_h5(filtered_matrix_h5) # this takes a few mins to read in the sparse matrix

filtered_feature_bc_matrix
# >>> filtered_feature_bc_matrix
# CountMatrix(feature_ref={'id': array([b'ENSMUSG00000051951', b'ENSMUSG00000089699',
#        b'ENSMUSG00000102343', ..., b'ENSMUSG00000063897',
#        b'ENSMUSG00000096730', b'ENSMUSG00000095742'], dtype='|S18'), 'name': array([b'Xkr4', b'Gm1992', b'Gm37381', ..., b'DHRSX', b'Vmn2r122',
#        b'CAAA01147332.1'], dtype='|S14')}, barcodes=array([b'AAACCTGAGATAGGAG-1', b'AAACCTGAGCGGCTTC-1',
#        b'AAACCTGAGGAATCGC-1', ..., b'TTTGTCAGTGCGATAG-133',
#        b'TTTGTCAGTTAAAGTG-133', b'TTTGTCATCTGAAAGA-133'], dtype='|S20'), matrix=<27998x1306127 sparse matrix of type '<class 'numpy.int32'>'
#         with 2624828308 stored elements in Compressed Sparse Column format>)
```

```{python}
filtered_feature_bc_matrix.matrix
# >>> filtered_feature_bc_matrix.matrix
# <27998x1306127 sparse matrix of type '<class 'numpy.int32'>'
#         with 2624828308 stored elements in Compressed Sparse Column format>
```

So let's try running mini-batch k-means on this sparse matrix 
```{python}
mbkm = MiniBatchKMeans(init='k-means++', n_clusters=`30, batch_size=100,
                      n_init=10, max_no_improvement=10, verbose=0)

# Update mbkmeans estimate on a single mini-batch X
t0 = time.time()
mbkm.partial_fit(filtered_feature_bc_matrix.matrix) 
t_mini_batch = time.time() - t0
# >>> mbkm.partial_fit(filtered_feature_bc_matrix.matrix)
# MemoryError: Unable to allocate array with shape (2624828308,) and data type int64

#  Compute the centroids on X by chunking it into mini-batches
t0 = time.time()
mbkm.fit(filtered_feature_bc_matrix.matrix) 
t_mini_batch = time.time() - t0
# >>> mbkm.fit(filtered_feature_bc_matrix.matrix)
# MemoryError: Unable to allocate array with shape (2624828308,) and data type int64
```

That did not work. Next I will try to use non-sparse representations. 


#### Load in the HDF5 file using `h5py` Python module

```{python}
# hdf5
import h5py

filename = "/fastscratch/myscratch/shicks1/benchmark-hdf5-clustering/main/case_studies/data/h5_python/1M_neurons_transposed_chunked.h5"
f = h5py.File(filename, 'r') # load hdf5 file
dat = f.get("counts")

mbkm = MiniBatchKMeans(n_clusters = 13, batch_size=100,compute_labels=True)
mbkm.output = mbkm.fit(dat) 
mbkm.output.cluster_centers_.shape # shape looks right

# output from MiniBatchKMeans
mbkm.output.cluster_centers_
mbkm.output.labels_
mbkm.output.inertia_
```


