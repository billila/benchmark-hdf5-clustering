---
title: "Figure 3: role of batch sizes"
author: "Ruoxi Liu"
date: "3/16/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, fig.width = 14, fig.height = 10)
```

```{r, message=FALSE, warning=FALSE}
library(tidyr)
library(ggplot2)
library(cowplot)
library(gridExtra)
library(dplyr)
library(grid)
library(here)
```

# Memory
```{r}
files_mem <- list.files(path= here("main", "case_studies", "output"), pattern="_mem_ruoxi_cluster.csv", full.names=TRUE, recursive=FALSE)
rl_table_mem <- NULL

for (i in (1:length(files_mem))){
  temp_table <- read.csv(file = files_mem[i], header=TRUE, sep=",")
  rl_table_mem <- rbind(rl_table_mem, temp_table)
}
```

```{r}
p_mem_obs <- rl_table_mem %>%
  dplyr::group_by(method, observations, abs_batch) %>% 
  dplyr::summarize(mean = mean(memory/1024), 
                   sd = sd(memory/1024)) %>% 
  ggplot(aes(x = abs_batch, y = mean, color = method)) +
          geom_point() +
          geom_errorbar(aes(ymin=mean-sd, ymax=mean+sd), width=.2,
                position=position_dodge(0.05))+
        labs(title = "Memory Usage for increasing number of cells and batch size") + 
        xlab("batch size") + 
        ylab("Memory Usage (Gb)") +
  facet_wrap(~observations) 
p_mem_obs
```
This doesn't make sense...

We expect memory consumption increases as batch size increases, and doesn't increases with number of observations.

### Memory - Zoom in
```{r}
p_mem_obs2 <- rl_table_mem %>%
  dplyr::filter((observations <= 100000)) %>%
  dplyr::group_by(method, observations, abs_batch) %>% 
  dplyr::summarize(mean = mean(memory/1024), 
                   sd = sd(memory/1024)) %>% 
  ggplot(aes(x = abs_batch, y = mean, color = method)) +
          #geom_line() +
          geom_point() +
          geom_errorbar(aes(ymin=mean-sd, ymax=mean+sd), width=.2,
                position=position_dodge(0.05))+
        labs(title = "Memory Usage for increasing number of batch size (75k data)") + 
        xlab("batch size") + 
        ylab("Memory Usage (Gb)") +
  facet_wrap(~observations) 
p_mem_obs2
```

```{r}
p_mem_obs3 <- rl_table_mem %>%
  dplyr::filter((observations >= 100000)) %>%
  dplyr::group_by(method, observations, abs_batch) %>% 
  dplyr::summarize(mean = mean(memory/1024), 
                   sd = sd(memory/1024)) %>% 
  ggplot(aes(x = abs_batch, y = mean, color = method)) +
          #geom_line() +
          geom_point() +
          geom_errorbar(aes(ymin=mean-sd, ymax=mean+sd), width=.2,
                position=position_dodge(0.05))+
        labs(title = "Memory Usage for increasing number of batch size (500k data)") + 
        xlab("batch size") + 
        ylab("Memory Usage (Gb)") +
  facet_wrap(~observations) 
p_mem_obs3
```
# Time
```{r}
files_rl <- list.files(path= here("main", "case_studies", "output"), pattern="_time_ruoxi_cluster.csv", full.names=TRUE, recursive=FALSE)
rl_table_time <- NULL

for (i in (1:length(files_rl))){
  temp_table <- read.csv(file = files_rl[i], header=TRUE, sep=",")
  rl_table_time <- rbind(rl_table_time, temp_table)
}
```

```{r}
p_time_obs <- rl_table_time %>%
  dplyr::group_by(method, observations, abs_batch) %>% 
  ggplot(aes(x = abs_batch, y = time3/60, color = method)) +
          geom_point() +
        labs(title = "Time for increasing number of cells and batch size") + 
        xlab("batch size") + 
        ylab("Time (Minutes)") + 
  facet_wrap(~observations) 
p_time_obs
```
Take aways:

1. hdf5 is slower than mbkmeans; kmeans is the slowest

2. time consumption increases as batch size increases